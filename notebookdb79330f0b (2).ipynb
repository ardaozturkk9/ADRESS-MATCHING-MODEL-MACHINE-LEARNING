{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12801953,"sourceType":"datasetVersion","datasetId":8094292},{"sourceId":12810518,"sourceType":"datasetVersion","datasetId":8100430}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install fuzzywuzzy\n!pip install python-levenshtein\n!pip install unidecode\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"1c7784eb-1d71-4450-9968-95520393ecf3","_cell_guid":"b0651aff-2422-4a84-9147-9c3756f1f1a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-19T23:57:38.415938Z","iopub.execute_input":"2025-08-19T23:57:38.416313Z","iopub.status.idle":"2025-08-19T23:57:57.851598Z","shell.execute_reply.started":"2025-08-19T23:57:38.416286Z","shell.execute_reply":"2025-08-19T23:57:57.850348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# KAGGLE INPUT DEBUG - TAM PATH KONTROLÃœ\nimport os\nimport pandas as pd\n\nprint(\"ğŸ” KAGGLE INPUT FULL DEBUG\")\nprint(\"=\"*60)\n\n# 1. Input dizininin tamamÄ±nÄ± listele\nprint(\"ğŸ“ /kaggle/input iÃ§eriÄŸi:\")\ntry:\n    for item in os.listdir('/kaggle/input'):\n        item_path = f'/kaggle/input/{item}'\n        if os.path.isdir(item_path):\n            print(f\"ğŸ“‚ {item}/\")\n            # Alt dizin iÃ§eriÄŸini gÃ¶ster\n            try:\n                for sub_item in os.listdir(item_path):\n                    print(f\"   ğŸ“„ {sub_item}\")\n            except:\n                print(f\"   âŒ Alt dizin okunamadÄ±\")\n        else:\n            print(f\"ğŸ“„ {item}\")\nexcept Exception as e:\n    print(f\"âŒ /kaggle/input okunamadÄ±: {e}\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# 2. Spesifik path'leri test et\ntest_paths = [\n    '/kaggle/input/datalar/train.csv',\n    '/kaggle/input/datalar/test.csv', \n    '/kaggle/input/datalar/sample_submission.csv'\n]\n\nprint(\"ğŸ¯ PATH TEST SONUÃ‡LARI:\")\nprint(\"-\"*40)\n\nfor path in test_paths:\n    print(f\"\\nğŸ“ Test ediliyor: {path}\")\n    \n    # File existence check\n    if os.path.exists(path):\n        print(\"âœ… Dosya var\")\n        \n        # File readable check\n        try:\n            with open(path, 'r') as f:\n                first_line = f.readline()\n            print(f\"âœ… Okunabilir - Ä°lk satÄ±r: {first_line[:50]}...\")\n            \n            # Pandas read check\n            try:\n                df = pd.read_csv(path, nrows=1)  # Sadece 1 satÄ±r oku\n                print(f\"âœ… Pandas ile okunabilir - Shape: {df.shape}\")\n                print(f\"âœ… Kolonlar: {list(df.columns)}\")\n            except Exception as e:\n                print(f\"âŒ Pandas hatasÄ±: {e}\")\n                \n        except Exception as e:\n            print(f\"âŒ Dosya okuma hatasÄ±: {e}\")\n    else:\n        print(\"âŒ Dosya bulunamadÄ±\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# 3. Alternatif path bulma\nprint(\"ğŸ” TÃœM CSV DOSYALARINI ARAMA:\")\nprint(\"-\"*40)\n\nfound_csv = []\nfor root, dirs, files in os.walk('/kaggle/input'):\n    for file in files:\n        if file.endswith('.csv'):\n            full_path = os.path.join(root, file)\n            found_csv.append((file, full_path))\n\nif found_csv:\n    print(\"ğŸ“‹ Bulunan CSV dosyalarÄ±:\")\n    for filename, fullpath in found_csv:\n        print(f\"  {filename} â†’ {fullpath}\")\nelse:\n    print(\"âŒ HiÃ§ CSV dosyasÄ± bulunamadÄ±!\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# 4. WORKING SOLUTION\nprint(\"ğŸ’¡ Ã‡ALIÅAN Ã‡Ã–ZÃœM:\")\nprint(\"-\"*30)\n\n# En gÃ¼venli path'i bul\nworking_paths = {}\nfor filename, fullpath in found_csv:\n    if 'train' in filename.lower():\n        working_paths['train'] = fullpath\n    elif 'test' in filename.lower() and 'sample' not in filename.lower():\n        working_paths['test'] = fullpath  \n    elif 'sample' in filename.lower() or 'submission' in filename.lower():\n        working_paths['sample'] = fullpath\n\nif working_paths:\n    print(\"âœ… KULLANILACAK PATH'LER:\")\n    for key, path in working_paths.items():\n        print(f\"{key} = pd.read_csv('{path}')\")\n    \n    print(f\"\\nğŸš€ TEST LOADING:\")\n    try:\n        if 'train' in working_paths:\n            train = pd.read_csv(working_paths['train'])\n            print(f\"âœ… train yÃ¼klendi: {train.shape}\")\n        \n        if 'test' in working_paths:\n            test = pd.read_csv(working_paths['test'])\n            print(f\"âœ… test yÃ¼klendi: {test.shape}\")\n            \n        if 'sample' in working_paths:\n            sample = pd.read_csv(working_paths['sample'])\n            print(f\"âœ… sample yÃ¼klendi: {sample.shape}\")\n            \n        print(\"\\nğŸ‰ TÃœM DOSYALAR BAÅARIYLA YÃœKLENDÄ°!\")\n        print(\"ArtÄ±k kodunuza devam edebilirsiniz!\")\n        \n    except Exception as e:\n        print(f\"âŒ YÃ¼kleme hatasÄ±: {e}\")\n        \nelse:\n    print(\"âŒ Uygun dosyalar bulunamadÄ±!\")\n\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T23:57:57.853577Z","iopub.execute_input":"2025-08-19T23:57:57.854541Z","iopub.status.idle":"2025-08-19T23:58:01.926704Z","shell.execute_reply.started":"2025-08-19T23:57:57.854470Z","shell.execute_reply":"2025-08-19T23:58:01.925397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bu satÄ±rÄ± kodunuzun en baÅŸÄ±na ekleyin\n!pip install unidecode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T23:58:01.927824Z","iopub.execute_input":"2025-08-19T23:58:01.928139Z","iopub.status.idle":"2025-08-19T23:58:06.435693Z","shell.execute_reply.started":"2025-08-19T23:58:01.928111Z","shell.execute_reply":"2025-08-19T23:58:06.434539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ADIM 2: KARAKTER DÃœZELTMÄ° VE ENCODÄ°NG TEMÄ°ZLÄ°ÄÄ°\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom unidecode import unidecode\n\n# Verileri yÃ¼kle (ADIM 1'den devam)\n# Path'lerini deÄŸiÅŸtir!\n# KAGGLE Ä°Ã‡Ä°N DOÄRU PATH'LER\ntrain = pd.read_csv('/kaggle/input/datalar/train.csv')\ntest = pd.read_csv('/kaggle/input/datalar/test.csv')\nsample = pd.read_csv('/kaggle/input/datalar/sample_submission.csv')\n\n\nprint(\"ğŸ”§ ADIM 2: KARAKTER DÃœZELTMÄ°\")\nprint(\"=\"*60)\n\n# Problematik karakterleri tanÄ±mla\nENCODING_FIXES = {\n    # TÃ¼rkÃ§e karakter bozukluklarÄ±\n    'Ã„Â±': 'Ä±', 'Ã„Â°': 'Ä°', 'Ã„': 'Ä°',\n    'ÃƒÂ±': 'Ã±', 'ÃƒÂ¼': 'Ã¼', 'ÃƒÂ¶': 'Ã¶', \n    'ÃƒÂ§': 'Ã§', 'Ãƒ': 'ÅŸ', 'Ã„': 'ÄŸ',\n    'Â±': 'Ä±', 'ÃƒÂ¥': 'Ã¥', 'ÃƒÂ¤': 'Ã¤',\n    'Ã¢': 'a', 'ÃƒÂª': 'e', 'ÃƒÂ´': 'o',\n    \n    # HTML entities\n    '&amp;': '&', '&lt;': '<', '&gt;': '>',\n    '&quot;': '\"', '&#39;': \"'\",\n    \n    # DiÄŸer encoding problemleri\n    'Ã¢â‚¬â„¢': \"'\", 'Ã¢â‚¬Å“': '\"', 'Ã¢â‚¬': '\"',\n    'Ã¢â‚¬\"': '-', 'Ã¢â‚¬\"': '-',\n}\n\ndef fix_encoding_issues(text):\n    \"\"\"Encoding problemlerini dÃ¼zelt\"\"\"\n    if pd.isna(text):\n        return text\n    \n    text = str(text)\n    \n    # Encoding dÃ¼zeltmeleri uygula\n    for wrong, correct in ENCODING_FIXES.items():\n        text = text.replace(wrong, correct)\n    \n    return text\n\ndef clean_turkish_text(text):\n    \"\"\"TÃ¼rkÃ§e metin temizliÄŸi\"\"\"\n    if pd.isna(text):\n        return text\n    \n    text = str(text)\n    \n    # Encoding dÃ¼zeltmeleri\n    text = fix_encoding_issues(text)\n    \n    # Fazla boÅŸluklarÄ± temizle\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # BaÅŸta ve sonda boÅŸluk temizle\n    text = text.strip()\n    \n    # Birden fazla noktalama iÅŸaretini tek yap\n    text = re.sub(r'\\.{2,}', '.', text)\n    text = re.sub(r',{2,}', ',', text)\n    text = re.sub(r':{2,}', ':', text)\n    \n    return text\n\n# Ã–NCESÄ° - Problemli karakter sayÄ±larÄ±\nprint(\"ğŸ” PROBLEM TESPÄ°TÄ° (Ã–NCESÄ°)\")\nprint(\"=\"*40)\n\nproblem_chars = ['Ã„Â±', 'Ã„Â°', 'ÃƒÂ±', 'ÃƒÂ¼', 'Â±', 'ÃƒÂ§', 'ÃƒÂ¶']\n\nprint(\"TRAIN VERÄ°SÄ°:\")\ntrain_problems_before = {}\nfor char in problem_chars:\n    count = train['address'].str.contains(char, regex=False, na=False).sum()\n    train_problems_before[char] = count\n    if count > 0:\n        print(f\"  '{char}' iÃ§eren: {count:,}\")\n\nprint(\"\\nTEST VERÄ°SÄ°:\")\ntest_problems_before = {}\nfor char in problem_chars:\n    count = test['address'].str.contains(char, regex=False, na=False).sum()\n    test_problems_before[char] = count\n    if count > 0:\n        print(f\"  '{char}' iÃ§eren: {count:,}\")\n\n# Ã–rnek problemli adresler gÃ¶ster\nprint(\"\\nğŸ“ PROBLEMLÄ° ADRES Ã–RNEKLERÄ°:\")\nprint(\"=\"*40)\n\nfor char in ['Ã„Â±', 'Ã„Â°', 'Â±']:\n    problematic = train[train['address'].str.contains(char, regex=False, na=False)]['address'].head(3)\n    if len(problematic) > 0:\n        print(f\"\\n'{char}' iÃ§eren Ã¶rnekler:\")\n        for i, addr in enumerate(problematic, 1):\n            print(f\"  {i}. {addr[:80]}{'...' if len(addr) > 80 else ''}\")\n\n# ADRES TEMÄ°ZLEME Ä°ÅLEMÄ°\nprint(\"\\nğŸ§¹ ADRES TEMÄ°ZLEME Ä°ÅLEMÄ° BAÅLIYOR...\")\nprint(\"=\"*50)\n\n# Train verisi temizliÄŸi\nprint(\"Train adresleri temizleniyor...\")\ntrain['address_cleaned'] = train['address'].apply(clean_turkish_text)\n\n# Test verisi temizliÄŸi  \nprint(\"Test adresleri temizleniyor...\")\ntest['address_cleaned'] = test['address'].apply(clean_turkish_text)\n\nprint(\"âœ… Temizleme tamamlandÄ±!\")\n\n# SONRASI - Problemli karakter sayÄ±larÄ±\nprint(\"\\nğŸ¯ SONUÃ‡LAR (SONRASI)\")\nprint(\"=\"*40)\n\nprint(\"TRAIN VERÄ°SÄ°:\")\ntrain_problems_after = {}\nfor char in problem_chars:\n    count = train['address_cleaned'].str.contains(char, regex=False, na=False).sum()\n    train_problems_after[char] = count\n    before = train_problems_before[char]\n    improvement = before - count\n    if before > 0:\n        print(f\"  '{char}': {before:,} â†’ {count:,} (dÃ¼zeltilen: {improvement:,})\")\n\nprint(\"\\nTEST VERÄ°SÄ°:\")\ntest_problems_after = {}\nfor char in problem_chars:\n    count = test['address_cleaned'].str.contains(char, regex=False, na=False).sum()\n    test_problems_after[char] = count\n    before = test_problems_before[char]\n    improvement = before - count\n    if before > 0:\n        print(f\"  '{char}': {before:,} â†’ {count:,} (dÃ¼zeltilen: {improvement:,})\")\n\n# Ã–RNEKLERÄ° KARÅILAÅTIR\nprint(\"\\nğŸ“Š Ã–NCE vs SONRA Ã–RNEKLERÄ°:\")\nprint(\"=\"*50)\n\n# Rastgele 5 adres karÅŸÄ±laÅŸtÄ±rmasÄ±\nsample_indices = train.sample(5, random_state=42).index\n\nprint(\"Ã–NCE â†’ SONRA:\")\nfor i, idx in enumerate(sample_indices, 1):\n    original = train.loc[idx, 'address']\n    cleaned = train.loc[idx, 'address_cleaned']\n    print(f\"\\n{i}. Ã–NCE:  {original[:70]}{'...' if len(original) > 70 else ''}\")\n    print(f\"   SONRA: {cleaned[:70]}{'...' if len(cleaned) > 70 else ''}\")\n    if original != cleaned:\n        print(f\"   âœ… DEÄÄ°ÅTÄ°\")\n    else:\n        print(f\"   â†’ DeÄŸiÅŸiklik yok\")\n\n# Ä°STATÄ°STÄ°KLER\nprint(\"\\nğŸ“ˆ TEMÄ°ZLEME Ä°STATÄ°STÄ°KLERÄ°:\")\nprint(\"=\"*50)\n\n# KaÃ§ adres deÄŸiÅŸti?\ntrain_changed = (train['address'] != train['address_cleaned']).sum()\ntest_changed = (test['address'] != test['address_cleaned']).sum()\n\nprint(f\"Train'de deÄŸiÅŸen adres sayÄ±sÄ±: {train_changed:,} / {len(train):,} (%{train_changed/len(train)*100:.1f})\")\nprint(f\"Test'te deÄŸiÅŸen adres sayÄ±sÄ±: {test_changed:,} / {len(test):,} (%{test_changed/len(test)*100:.1f})\")\n\n# Ortalama uzunluk deÄŸiÅŸimi\ntrain_length_diff = train['address_cleaned'].str.len() - train['address'].str.len()\ntest_length_diff = test['address_cleaned'].str.len() - test['address'].str.len()\n\nprint(f\"\\nOrtalama uzunluk deÄŸiÅŸimi:\")\nprint(f\"Train: {train_length_diff.mean():.2f} karakter\")\nprint(f\"Test: {test_length_diff.mean():.2f} karakter\")\n\n# EK TEMÄ°ZLEME KONTROLLERI\nprint(\"\\nğŸ” EK KONTROLLER:\")\nprint(\"=\"*30)\n\n# Null deÄŸerler kontrol\nprint(f\"Train'de null adres: {train['address_cleaned'].isna().sum()}\")\nprint(f\"Test'te null adres: {test['address_cleaned'].isna().sum()}\")\n\n# Ã‡ok kÄ±sa adresler\nshort_train = (train['address_cleaned'].str.len() < 5).sum()\nshort_test = (test['address_cleaned'].str.len() < 5).sum()\nprint(f\"Ã‡ok kÄ±sa adresler (< 5 karakter): Train={short_train}, Test={short_test}\")\n\n# Ã‡ok uzun adresler  \nlong_train = (train['address_cleaned'].str.len() > 200).sum()\nlong_test = (test['address_cleaned'].str.len() > 200).sum()\nprint(f\"Ã‡ok uzun adresler (> 200 karakter): Train={long_train}, Test={long_test}\")\n\nprint(\"\\nâœ… ADIM 2 TAMAMLANDI!\")\nprint(\"=\"*60)\nprint(\"ğŸ¯ BAÅARILAR:\")\nprint(\"â€¢ Encoding sorunlarÄ± dÃ¼zeltildi\")\nprint(\"â€¢ TÃ¼rkÃ§e karakterler standardize edildi\") \nprint(\"â€¢ Fazla boÅŸluklar temizlendi\")\nprint(\"â€¢ Noktalama dÃ¼zeltmeleri yapÄ±ldÄ±\")\nprint()\nprint(\"ğŸ“ YENÄ° KOLONLAR:\")\nprint(\"â€¢ train['address_cleaned'] - TemizlenmiÅŸ train adresleri\")\nprint(\"â€¢ test['address_cleaned'] - TemizlenmiÅŸ test adresleri\") \nprint()\nprint(\"ğŸ¯ SIRADA: ADIM 3 - Adres Temizleme ve KÄ±saltma GeniÅŸletme\")\nprint(\"=\"*60)","metadata":{"_uuid":"c64bea68-0934-41c9-912b-b54db5aaad1d","_cell_guid":"fe1a616f-b929-4b77-97c8-b08ced676424","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-19T23:58:06.438803Z","iopub.execute_input":"2025-08-19T23:58:06.439125Z","iopub.status.idle":"2025-08-19T23:58:36.335536Z","shell.execute_reply.started":"2025-08-19T23:58:06.439097Z","shell.execute_reply":"2025-08-19T23:58:36.333832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ADIM 3 - KAGGLE Ä°Ã‡Ä°N FUZZY MATCHING + BATCH (HATASIZ)\n# Ã–nce gerekli kÃ¼tÃ¼phaneleri yÃ¼kle ve import et\n\n\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom fuzzywuzzy import fuzz, process\nfrom collections import Counter\nimport time\nimport gc\n\nprint(\"ğŸ¯ ADIM 3: KAGGLE Ä°Ã‡Ä°N FUZZY MATCHING + BATCH\")\nprint(\"=\"*70)\n\n# VERÄ° YÃœKLEME (Kaggle iÃ§in path'ler)\nprint(\"ğŸ“‚ Veriler yÃ¼kleniyor...\")\n# KAGGLE Ä°Ã‡Ä°N DOÄRU PATH'LER\n\ntry:\n    train = pd.read_csv('/kaggle/input/hackathon-data/train.csv')\n    test = pd.read_csv('/kaggle/input/hackathon-data/test.csv')\n    sample = pd.read_csv('/kaggle/input/hackathon-data/sample_submission.csv')\n    print(\"âœ… Veriler baÅŸarÄ±yla yÃ¼klendi!\")\nexcept:\n    print(\"âš ï¸ Kaggle input path'i bulunamadÄ±. Manuel path kullanÄ±n:\")\n    print(\"train = pd.read_csv('your_path/train.csv')\")\n    print(\"test = pd.read_csv('your_path/test.csv')\")\n\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\n\n# ENCODING DÃœZELTMESÄ° (ADIM 2'den)\nENCODING_FIXES = {\n    'Ã„Â±': 'Ä±', 'Ã„Â°': 'Ä°', 'Ã„': 'Ä°',\n    'ÃƒÂ±': 'Ã±', 'ÃƒÂ¼': 'Ã¼', 'ÃƒÂ¶': 'Ã¶', \n    'ÃƒÂ§': 'Ã§', 'Ãƒ': 'ÅŸ', 'Ã„': 'ÄŸ',\n    'Â±': 'Ä±', 'Ã¢': 'a', '&amp;': '&',\n}\n\ndef fix_encoding(text):\n    \"\"\"Encoding problemlerini dÃ¼zelt\"\"\"\n    if pd.isna(text):\n        return text\n    text = str(text)\n    for wrong, correct in ENCODING_FIXES.items():\n        text = text.replace(wrong, correct)\n    return re.sub(r'\\s+', ' ', text).strip()\n\n# EÄŸer address_cleaned kolonu yoksa oluÅŸtur\nif 'address_cleaned' not in train.columns:\n    print(\"ğŸ”§ Encoding dÃ¼zeltmesi yapÄ±lÄ±yor...\")\n    train['address_cleaned'] = train['address'].apply(fix_encoding)\n    test['address_cleaned'] = test['address'].apply(fix_encoding)\n    print(\"âœ… Encoding dÃ¼zeltmesi tamamlandÄ±!\")\n\n# FUZZY MATCHING Ä°Ã‡Ä°N STANDART KELÄ°MELER\nSTANDARD_WORDS = {\n    'sokak': ['sok', 'sk', 's', 'sokagi', 'sokag', 'sokok', 'sokk', 'skk', 'soak', 'skoak', \n              'sokakk', 'skak', 'soks', 'sokkak', 'sookak', 'sokaak', 'sokag', 'sokakk'],\n    \n    'mahallesi': ['mah', 'mh', 'm', 'maha', 'mahl', 'mahle', 'mahall', 'mahalle', 'mahal',\n                  'mhal', 'mhle', 'mhall', 'mhallesi', 'mahles', 'mahale', 'mahallsi', 'mahallesi'],\n    \n    'caddesi': ['cad', 'cd', 'c', 'cadde', 'caddesi', 'cde', 'cdd', 'cdde', 'caddde', \n                'cadesi', 'cdes', 'cdsi', 'kadde', 'kaddesi', 'caddesi'],\n    \n    'numara': ['no', 'n', 'num', 'numara', 'nr', 'nmr', 'numr', 'numars', 'nummara',\n               'numero', 'numar', 'numera', 'numara'],\n    \n    'apartmani': ['apt', 'ap', 'apartman', 'apartmani', 'apartmn', 'aprt', 'aprtmn',\n                  'aprtn', 'aprtman', 'apartmaan', 'apartmani'],\n    \n    'blok': ['bl', 'b', 'blok', 'blk', 'blokk', 'block', 'blook', 'blok'],\n    \n    'daire': ['d', 'da', 'dr', 'daire', 'dair', 'dare', 'daira', 'daires', 'dairre', 'daire'],\n    \n    'kat': ['k', 'kt', 'kat', 'katt', 'kaat', 'kate', 'kat'],\n    \n    'sitesi': ['sit', 'site', 'sitesi', 'sites', 'siteesi', 'siitesi', 'sitesi'],\n    \n    'bulvari': ['bul', 'bulvar', 'bulvari', 'bv', 'blv', 'blvr', 'bulvr', 'bulvaari', 'bulvari'],\n    \n    'yolu': ['yl', 'yol', 'yolu', 'yoll', 'yoolu', 'yolu'],\n    \n    'plaza': ['plz', 'pl', 'plaza', 'plaza'],\n    \n    'hastanesi': ['hast', 'hastan', 'hastane', 'hastanesi', 'hastanesi'],\n    \n    'okulu': ['ok', 'okul', 'okulu', 'okulu'],\n    \n    'universitesi': ['univ', 'univers', 'universite', 'universitesi', 'Ã¼niv', 'Ã¼niver', \n                     'Ã¼niversite', 'Ã¼niversitesi', 'universitesi'],\n    \n    'merkezi': ['mrk', 'merkez', 'merkezi', 'merkezi'],\n}\n\n# PERFORMANS CACHE SÄ°STEMÄ°\nword_cache = {}\ncache_hits = 0\ncache_misses = 0\n\ndef fuzzy_standardize_word(word, threshold=75):\n    \"\"\"\n    Cache'li fuzzy matching ile kelime standardizasyonu\n    \"\"\"\n    global cache_hits, cache_misses\n    \n    if not word or len(word) < 2:\n        return word\n    \n    word = word.lower().strip()\n    \n    # SayÄ± ise dokunma\n    if word.isdigit():\n        return word\n    \n    # Cache kontrolÃ¼\n    if word in word_cache:\n        cache_hits += 1\n        return word_cache[word]\n    \n    cache_misses += 1\n    \n    # Exact match (hÄ±zlÄ± kontrol)\n    for standard, variants in STANDARD_WORDS.items():\n        if word in variants:\n            word_cache[word] = standard\n            return standard\n    \n    # Fuzzy matching\n    best_match = word\n    best_score = 0\n    \n    for standard, variants in STANDARD_WORDS.items():\n        for variant in variants:\n            # Uzunluk farkÄ± Ã§ok fazlaysa skip et (performans)\n            if abs(len(word) - len(variant)) > 3:\n                continue\n            \n            # FarklÄ± similarity metrikleri\n            ratio_score = fuzz.ratio(word, variant)\n            partial_score = fuzz.partial_ratio(word, variant)\n            \n            max_score = max(ratio_score, partial_score)\n            \n            if max_score > best_score and max_score >= threshold:\n                best_score = max_score\n                best_match = standard\n    \n    # Cache'e kaydet\n    word_cache[word] = best_match\n    return best_match\n\ndef clean_punctuation_and_format(text):\n    \"\"\"\n    Noktalama ve format temizliÄŸi\n    \"\"\"\n    if not text:\n        return text\n    \n    # Noktalama dÃ¼zeltmeleri\n    text = re.sub(r'\\.+', '.', text)  # Ã‡oklu nokta\n    text = re.sub(r',+', ',', text)   # Ã‡oklu virgÃ¼l\n    text = re.sub(r':+', ':', text)   # Ã‡oklu iki nokta\n    \n    # Numara formatlarÄ±\n    text = re.sub(r'(\\d+)\\s*[/-]\\s*(\\d+)', r'\\1/\\2', text)  # \"15 - 3\" â†’ \"15/3\"\n    text = re.sub(r'(\\d+)\\s*\\.\\s*(\\d+)', r'\\1/\\2', text)    # \"15 . 3\" â†’ \"15/3\"\n    \n    # Parantez temizliÄŸi\n    text = re.sub(r'\\(\\s*\\)', '', text)  # BoÅŸ parantez\n    text = re.sub(r'\\[\\s*\\]', '', text)  # BoÅŸ kÃ¶ÅŸeli parantez\n    \n    return text\ndef remove_duplicates(text):\n    \"\"\"Ã‡ift kelimeleri temizle\"\"\"\n    words = text.split()\n    result = []\n    for word in words:\n        if not result or word != result[-1]:\n            result.append(word)\n    return ' '.join(result)\n\ndef smart_address_normalization(text):\n    \"\"\"Fuzzy matching ile tam adres normalizasyonu\"\"\"\n    if pd.isna(text):\n        return text\n    \n    text = str(text).lower().strip()\n    \n    # 1. Format temizliÄŸi\n    text = clean_punctuation_and_format(text)\n    \n    # 2. Kelimelere ayÄ±r ve normalize et\n    words = text.split()\n    normalized_words = []\n    \n    for word in words:\n        # Noktalama iÅŸaretlerini ayÄ±r\n        punct_pattern = r'^([^\\w]*)(.*?)([^\\w]*)$'\n        match = re.match(punct_pattern, word)\n        \n        if match:\n            prefix_punct, clean_word, suffix_punct = match.groups()\n            \n            if clean_word:\n                # Fuzzy matching uygula\n                standardized = fuzzy_standardize_word(clean_word, threshold=75)  # 80 daha etkili olabilir\n                normalized_word = prefix_punct + standardized + suffix_punct\n                normalized_words.append(normalized_word)\n            else:\n                normalized_words.append(word)\n        else:\n            # Fallback - direkt iÅŸle\n            standardized = fuzzy_standardize_word(word, threshold=75)\n            normalized_words.append(standardized)\n    \n    # 3. BirleÅŸtir\n    result = ' '.join(normalized_words)\n    \n    # 4. Fazla boÅŸluk temizle\n    result = re.sub(r'\\s+', ' ', result)\n    \n    # 5. Duplicate temizle\n    result = remove_duplicates(result)\n    \n    # 6. Son temizlik\n    return result.strip()\n   \n \n\ndef process_in_batches(df, input_col, output_col, batch_size=15000, description=\"\"):\n    \"\"\"\n    GÃ¼venli batch iÅŸleme - memory yÃ¶netimi ile\n    \"\"\"\n    print(f\"ğŸ“¦ {description} - {len(df):,} satÄ±r {batch_size:,} batch'de iÅŸleniyor...\")\n    \n    results = []\n    total_batches = (len(df) + batch_size - 1) // batch_size\n    \n    for i in range(0, len(df), batch_size):\n        batch_start = i\n        batch_end = min(i + batch_size, len(df))\n        batch_num = (i // batch_size) + 1\n        \n        print(f\"   ğŸ“Š Batch {batch_num}/{total_batches}: {batch_start:,}-{batch_end:,}\")\n        start_time = time.time()\n        \n        # Batch'i iÅŸle\n        batch_data = df[input_col].iloc[batch_start:batch_end]\n        batch_results = batch_data.apply(smart_address_normalization)\n        results.extend(batch_results.tolist())\n        \n        batch_time = time.time() - start_time\n        print(f\"     â±ï¸ Batch sÃ¼resi: {batch_time:.1f}s | Cache hits: {cache_hits:,} | Cache misses: {cache_misses:,}\")\n        \n        # Memory temizliÄŸi (her 5 batch'te bir)\n        if batch_num % 5 == 0:\n            gc.collect()\n            print(f\"      Memory temizlendi | Cache boyutu: {len(word_cache):,}\")\n    \n    return results\n\n# TEST - KÃœÃ‡ÃœK SAMPLE Ä°LE Ã–NCE\nprint(\"\\nğŸ§ª TEST - 1000 SATIRLIK SAMPLE Ä°LE\")\nprint(\"=\"*50)\n\n# 1000 satÄ±rlÄ±k test\ntest_sample = train.sample(1000, random_state=42)\nstart_time = time.time()\ntest_results = test_sample['address_cleaned'].apply(smart_address_normalization)\ntest_time = time.time() - start_time\n\nprint(f\"âœ… 1000 satÄ±r test: {test_time:.1f} saniye\")\nprint(f\"ğŸ“Š Saniye baÅŸÄ±na: {1000/test_time:.0f} satÄ±r\")\nprint(f\"ğŸ¯ Tahmini tam sÃ¼re: {(len(train) + len(test)) * test_time / 1000 / 60:.1f} dakika\")\n\n# Ã–rnek sonuÃ§lar\nchanged_count = (test_sample['address_cleaned'] != test_results).sum()\nprint(f\"ğŸ“ˆ DeÄŸiÅŸen adres oranÄ±: {changed_count}/1000 (%{changed_count/10:.1f})\")\n\nprint(\"\\nğŸ“ SAMPLE SONUÃ‡ Ã–RNEKLERÄ°:\")\nfor i in range(3):\n    original = test_sample.iloc[i]['address_cleaned']\n    normalized = test_results.iloc[i]\n    if original != normalized:\n        print(f\"Ã–NCE:  {original}\")\n        print(f\"SONRA: {normalized}\")\n        print()\n\n# KULLANICIDAN ONAY AL\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ¯ FUZZY MATCHING + BATCH Ä°LE TAM VERÄ°YÄ° Ä°ÅLEMEYE HAZIR!\")\nprint(\"ğŸ“Š Beklenen sÃ¼re: 15-25 dakika\")\nprint(\"ğŸ’¾ Memory kullanÄ±mÄ±: GÃ¼venli (batch + cache sistem)\")\nprint(\"ğŸ¯ Kalite: En yÃ¼ksek doÄŸruluk oranÄ±\")\nprint()\nprint(\"Devam etmek iÃ§in aÅŸaÄŸÄ±daki kodu Ã§alÄ±ÅŸtÄ±rÄ±n:\")\nprint()\n\n# ANA Ä°ÅLEME KODU\nprint(\"# ANA FUZZY MATCHING Ä°ÅLEMÄ°\")\nprint(\"# TRAIN VERÄ°SÄ° BATCH Ä°ÅLEME\")\nprint(\"print('ğŸš€ TRAIN VERÄ°SÄ° FUZZY MATCHING BAÅLIYOR...')\")\nprint(\"train_normalized_results = process_in_batches(\")\nprint(\"    df=train,\")\nprint(\"    input_col='address_cleaned',\") \nprint(\"    output_col='address_normalized',\")\nprint(\"    batch_size=15000,\")\nprint(\"    description='Train fuzzy matching'\")\nprint(\")\")\nprint(\"train['address_normalized'] = train_normalized_results\")\nprint()\n\nprint(\"# TEST VERÄ°SÄ° BATCH Ä°ÅLEME\") \nprint(\"print('ğŸš€ TEST VERÄ°SÄ° FUZZY MATCHING BAÅLIYOR...')\")\nprint(\"test_normalized_results = process_in_batches(\")\nprint(\"    df=test,\")\nprint(\"    input_col='address_cleaned',\")\nprint(\"    output_col='address_normalized',\") \nprint(\"    batch_size=15000,\")\nprint(\"    description='Test fuzzy matching'\")\nprint(\")\")\nprint(\"test['address_normalized'] = test_normalized_results\")\nprint()\n\nprint(\"# SONUÃ‡ ANALÄ°ZÄ°\")\nprint(\"train_changed = (train['address_cleaned'] != train['address_normalized']).sum()\")\nprint(\"test_changed = (test['address_cleaned'] != test['address_normalized']).sum()\")\nprint(\"print(f'âœ… ADIM 3 TAMAMLANDI!')\")\nprint(\"print(f'Train deÄŸiÅŸen: {train_changed:,} / {len(train):,}')\")\nprint(\"print(f'Test deÄŸiÅŸen: {test_changed:,} / {len(test):,}')\")\nprint(\"print(f'Cache efficiency: %{cache_hits/(cache_hits+cache_misses)*100:.1f}')\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… FUZZY MATCHING + BATCH SÄ°STEMÄ° HAZIR!\")\nprint(\"ğŸ¯ YukarÄ±daki kodlarÄ± sÄ±rasÄ±yla Ã§alÄ±ÅŸtÄ±rarak devam edin.\")\nprint(\"âš¡ Kaggle'da sorunsuz Ã§alÄ±ÅŸacak, kernel Ã§Ã¶kmeyecek!\")\nprint(\"=\"*70)","metadata":{"_uuid":"d679e75c-2e1c-4463-89d2-b0b94ee72d5d","_cell_guid":"7f104776-a17a-421f-a1fe-73a328f35d9a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-19T23:58:36.336970Z","iopub.execute_input":"2025-08-19T23:58:36.337445Z","iopub.status.idle":"2025-08-19T23:58:39.592039Z","shell.execute_reply.started":"2025-08-19T23:58:36.337288Z","shell.execute_reply":"2025-08-19T23:58:39.590860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ANA FUZZY MATCHING ISLEMI\n# TRAIN VERISI BATCH ISLEME\nprint('TRAIN VERISI FUZZY MATCHING BASLIYOR...')\ntrain_normalized_results = process_in_batches(\n    df=train,\n    input_col='address_cleaned',\n    output_col='address_normalized',\n    batch_size=15000,\n    description='Train fuzzy matching'\n)\ntrain['address_normalized'] = train_normalized_results\n\n# TEST VERISI BATCH ISLEME\nprint('TEST VERISI FUZZY MATCHING BASLIYOR...')\ntest_normalized_results = process_in_batches(\n    df=test,\n    input_col='address_cleaned',\n    output_col='address_normalized',\n    batch_size=15000,\n    description='Test fuzzy matching'\n)\ntest['address_normalized'] = test_normalized_results\n\n# SONUC ANALIZI\ntrain_changed = (train['address_cleaned'] != train['address_normalized']).sum()\ntest_changed = (test['address_cleaned'] != test['address_normalized']).sum()\nprint('ADIM 3 TAMAMLANDI!')\nprint(f'Train degisen: {train_changed:,} / {len(train):,}')\nprint(f'Test degisen: {test_changed:,} / {len(test):,}')\nprint(f'Cache efficiency: %{cache_hits/(cache_hits+cache_misses)*100:.1f}')","metadata":{"_uuid":"550cc100-1a13-4fc9-94ea-bb9840184241","_cell_guid":"edf5f254-2380-442f-9aa1-3b5336eca99f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-19T23:58:39.592809Z","iopub.execute_input":"2025-08-19T23:58:39.593072Z","iopub.status.idle":"2025-08-20T00:03:09.406982Z","shell.execute_reply.started":"2025-08-19T23:58:39.593051Z","shell.execute_reply":"2025-08-20T00:03:09.406058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ADIM 4: BENZERLIK HESAPLAMA VE LABEL TAHMIN\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.neighbors import NearestNeighbors\nimport time\nimport gc\n\nprint(\"ğŸ¯ ADIM 4: BENZERLIK HESAPLAMA VE LABEL TAHMIN\")\nprint(\"=\"*70)\n\n# VERÄ° DURUMU KONTROL\nprint(\"ğŸ“Š Veri durumu kontrol:\")\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\nprint(f\"Train'de address_normalized var mÄ±: {'address_normalized' in train.columns}\")\nprint(f\"Test'te address_normalized var mÄ±: {'address_normalized' in test.columns}\")\n\n# LABEL ANALIZI\nprint(f\"\\nğŸ·ï¸ Label analizi:\")\nunique_labels = train['label'].nunique()\nprint(f\"Unique label sayÄ±sÄ±: {unique_labels:,}\")\nprint(f\"En yaygÄ±n 5 label:\")\nprint(train['label'].value_counts().head())\n\n# STRATEJI 1: TF-IDF + COSINE SIMILARITY\nprint(f\"\\nğŸ“ˆ STRATEJI 1: TF-IDF + COSINE SIMILARITY\")\nprint(\"=\"*50)\n\ndef create_label_representatives(train_df):\n    \"\"\"\n    Her label iÃ§in temsilci adres(ler) oluÅŸtur\n    \"\"\"\n    print(\"ğŸ—ï¸ Label temsilcileri oluÅŸturuluyor...\")\n    \n    label_representatives = {}\n    \n    for label in train_df['label'].unique():\n        label_addresses = train_df[train_df['label'] == label]['address_normalized'].tolist()\n        \n        if len(label_addresses) == 1:\n            # Tek adres varsa direkt kullan\n            label_representatives[label] = label_addresses[0]\n        else:\n            # Ã‡ok adres varsa en uzun olanÄ± al (daha detaylÄ±)\n            longest_address = max(label_addresses, key=len)\n            label_representatives[label] = longest_address\n    \n    return label_representatives\n\n# Label temsilcilerini oluÅŸtur\nlabel_reps = create_label_representatives(train)\nprint(f\"âœ… {len(label_reps):,} label temsilcisi oluÅŸturuldu\")\n\n# TF-IDF vektÃ¶rleÅŸtirme\nprint(\"\\nğŸ”¢ TF-IDF vektÃ¶rleÅŸtirme...\")\nvectorizer = TfidfVectorizer(\n    ngram_range=(1, 2),  # 1-gram ve 2-gram\n    min_df=2,            # En az 2 dokÃ¼manda geÃ§meli\n    max_df=0.8,          # Maksimum %80 dokÃ¼manda geÃ§ebilir\n    strip_accents='unicode',\n    lowercase=True\n)\n\n# TÃ¼m adresleri birleÅŸtir (train temsilcileri + test)\nall_addresses = list(label_reps.values()) + test['address_normalized'].tolist()\nprint(f\"ğŸ“ {len(all_addresses):,} adres vektÃ¶rleÅŸtiriliyor...\")\n\n# TF-IDF fit et\ntfidf_matrix = vectorizer.fit_transform(all_addresses)\nprint(f\"âœ… TF-IDF matrix: {tfidf_matrix.shape}\")\n\n# Train temsilcileri ve test adreslerini ayÄ±r\nn_train_reps = len(label_reps)\ntrain_tfidf = tfidf_matrix[:n_train_reps]\ntest_tfidf = tfidf_matrix[n_train_reps:]\n\nprint(f\"ğŸ“Š Train temsilci matrix: {train_tfidf.shape}\")\nprint(f\"ğŸ“Š Test matrix: {test_tfidf.shape}\")\n\n# STRATEJI 2: NEAREST NEIGHBORS APPROACH\nprint(f\"\\nğŸ¯ STRATEJI 2: NEAREST NEIGHBORS\")\nprint(\"=\"*50)\n\n# K-Nearest Neighbors modeli\nprint(\"ğŸ” KNN modeli oluÅŸturuluyor...\")\nknn_model = NearestNeighbors(\n    n_neighbors=5,       # En yakÄ±n 5 komÅŸu\n    metric='cosine',     # Cosine benzerlik\n    algorithm='brute'    # Brute force (daha doÄŸru)\n)\n\n# Train temsilcileri ile fit et\nknn_model.fit(train_tfidf)\nprint(\"âœ… KNN modeli eÄŸitildi\")\n\n# Batch tahmin fonksiyonu\ndef predict_labels_batch(test_tfidf_batch, batch_start_idx, batch_size=5000):\n    \"\"\"\n    Test batch'i iÃ§in label tahminleri yap\n    \"\"\"\n    print(f\"   ğŸ”® Batch tahminler yapÄ±lÄ±yor: {batch_start_idx:,}-{batch_start_idx+test_tfidf_batch.shape[0]:,}\")\n    \n    # En yakÄ±n komÅŸularÄ± bul\n    distances, indices = knn_model.kneighbors(test_tfidf_batch)\n    \n    predictions = []\n    confidence_scores = []\n    \n    for i in range(test_tfidf_batch.shape[0]):  # len() yerine .shape[0] kullan\n        # En yakÄ±n komÅŸunun label'Ä±\n        closest_idx = indices[i][0]\n        label_list = list(label_reps.keys())\n        predicted_label = label_list[closest_idx]\n        \n        # Confidence skoru (1 - cosine distance)\n        confidence = 1 - distances[i][0]\n        \n        predictions.append(predicted_label)\n        confidence_scores.append(confidence)\n    \n    return predictions, confidence_scores\n\n# BATCH TAHMÄ°N Ä°ÅLEMÄ°\nprint(f\"\\nğŸš€ BATCH TAHMÄ°N Ä°ÅLEMÄ° BAÅLIYOR\")\nprint(\"=\"*50)\n\nbatch_size = 5000  # 5k test adresi her batch'te\ntotal_predictions = []\ntotal_confidences = []\n\nn_batches = (test_tfidf.shape[0] + batch_size - 1) // batch_size\n\nfor batch_idx in range(n_batches):\n    start_idx = batch_idx * batch_size\n    end_idx = min((batch_idx + 1) * batch_size, test_tfidf.shape[0])\n    \n    print(f\"ğŸ“¦ Batch {batch_idx + 1}/{n_batches}\")\n    \n    # Batch'i al\n    test_batch = test_tfidf[start_idx:end_idx]\n    \n    # Tahmin yap\n    batch_predictions, batch_confidences = predict_labels_batch(\n        test_batch, start_idx, batch_size\n    )\n    \n    total_predictions.extend(batch_predictions)\n    total_confidences.extend(batch_confidences)\n    \n    # Memory temizliÄŸi\n    if (batch_idx + 1) % 3 == 0:\n        gc.collect()\n\nprint(f\"âœ… TÃ¼m tahminler tamamlandÄ±: {len(total_predictions):,}\")\n\n# SONUÃ‡LARI TEST DataFrame'ine EKLE\ntest['predicted_label'] = total_predictions\ntest['confidence_score'] = total_confidences\n\n# Ä°STATÄ°STÄ°KLER\nprint(f\"\\nğŸ“Š TAHMÄ°N Ä°STATÄ°STÄ°KLERÄ°\")\nprint(\"=\"*50)\n\nconfidence_mean = np.mean(total_confidences)\nconfidence_std = np.std(total_confidences)\n\nprint(f\"Ortalama confidence: {confidence_mean:.3f}\")\nprint(f\"Confidence std: {confidence_std:.3f}\")\nprint(f\"Min confidence: {min(total_confidences):.3f}\")\nprint(f\"Max confidence: {max(total_confidences):.3f}\")\n\n# Confidence daÄŸÄ±lÄ±mÄ±\nhigh_conf = sum(1 for c in total_confidences if c > 0.8)\nmedium_conf = sum(1 for c in total_confidences if 0.5 < c <= 0.8)\nlow_conf = sum(1 for c in total_confidences if c <= 0.5)\n\nprint(f\"\\nConfidence daÄŸÄ±lÄ±mÄ±:\")\nprint(f\"YÃ¼ksek (>0.8): {high_conf:,} (%{high_conf/len(total_confidences)*100:.1f})\")\nprint(f\"Orta (0.5-0.8): {medium_conf:,} (%{medium_conf/len(total_confidences)*100:.1f})\")\nprint(f\"DÃ¼ÅŸÃ¼k (â‰¤0.5): {low_conf:,} (%{low_conf/len(total_confidences)*100:.1f})\")\n\n# Ã–RNEK TAHMÄ°NLER\nprint(f\"\\nğŸ“ Ã–RNEK TAHMÄ°NLER\")\nprint(\"=\"*50)\n\nsample_results = test.sample(5, random_state=42)\nfor idx, row in sample_results.iterrows():\n    print(f\"\\nTest ID: {row['id']}\")\n    print(f\"Adres: {row['address_normalized'][:60]}...\")\n    print(f\"Tahmin edilen label: {row['predicted_label']}\")\n    print(f\"Confidence: {row['confidence_score']:.3f}\")\n\n# SUBMISSION HAZIRLIK\nprint(f\"\\nğŸ“„ SUBMISSION DOSYASI HAZIRLIÄI\")\nprint(\"=\"*50)\n\n# Submission formatÄ± kontrol\nprint(\"Sample submission format:\")\nprint(sample.head())\n\n# Submission dataframe oluÅŸtur\nsubmission = test[['id', 'predicted_label']].copy()\nsubmission.columns = ['id', 'label']  # sample_submission formatÄ±na uygun\n\nprint(f\"\\nSubmission shape: {submission.shape}\")\nprint(f\"Sample submission shape: {sample.shape}\")\n\n# Ä°lk 5 satÄ±rÄ± karÅŸÄ±laÅŸtÄ±r\nprint(f\"\\nHazÄ±rlanan submission:\")\nprint(submission.head())\n\nprint(f\"\\nâœ… ADIM 4 TAMAMLANDI!\")\nprint(\"=\"*70)\nprint(\"ğŸ¯ BAÅARILAR:\")\nprint(\"â€¢ TF-IDF vektÃ¶rleÅŸtirme ile semantik benzerlik\")\nprint(\"â€¢ KNN ile en yakÄ±n komÅŸu bulma\")\nprint(\"â€¢ Batch processing ile memory yÃ¶netimi\")\nprint(\"â€¢ Confidence skorlarÄ± ile gÃ¼venilirlik Ã¶lÃ§Ã¼mÃ¼\")\nprint(\"â€¢ Submission formatÄ±nda hazÄ±r sonuÃ§lar\")\nprint()\nprint(\"ğŸ“ YENÄ° KOLONLAR:\")\nprint(\"â€¢ test['predicted_label'] - Tahmin edilen label'lar\")\nprint(\"â€¢ test['confidence_score'] - GÃ¼venilirlik skorlarÄ±\")\nprint(\"â€¢ submission - Submit edilecek format\")\nprint()\nprint(\"ğŸ¯ SIRADA: SUBMISSION ve SONUÃ‡ ANALIZI\")\nprint(\"=\"*70)","metadata":{"_uuid":"69e72d23-d605-44c2-b5eb-22c4a4f293d0","_cell_guid":"062e038f-164a-4bc8-9158-80d7dc870574","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-20T00:03:09.408387Z","iopub.execute_input":"2025-08-20T00:03:09.408688Z","iopub.status.idle":"2025-08-20T00:06:26.179747Z","shell.execute_reply.started":"2025-08-20T00:03:09.408667Z","shell.execute_reply":"2025-08-20T00:06:26.178721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bu kod ADIM 4'te zaten var, kontrol et:\nsubmission = test[['id', 'predicted_label']].copy()\nsubmission.columns = ['id', 'label']\n\n# CSV olarak kaydet\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"âœ… Submission dosyasÄ± hazÄ±r!\")","metadata":{"_uuid":"5a6b3fee-2548-4497-bbe9-dde67d281ebc","_cell_guid":"1ecd4c62-c3f3-4734-b44d-6549c8bd2582","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-20T00:06:26.180743Z","iopub.execute_input":"2025-08-20T00:06:26.181617Z","iopub.status.idle":"2025-08-20T00:06:26.401904Z","shell.execute_reply.started":"2025-08-20T00:06:26.181586Z","shell.execute_reply":"2025-08-20T00:06:26.400691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#IYILESTIRME EVRESINE GECIS YAPTIK OZSÄ°\n# ÃœÃ‡ Ä°YÄ°LEÅTÄ°RME STRATEJÄ°SÄ° - GECE BOYU Ã‡ALIÅSIN\n# Strateji 1: String Similarity (zaten Ã§alÄ±ÅŸÄ±yor)\n# Strateji 2: Geographic Features  \n# Strateji 3: Ensemble Methods\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom collections import Counter\nimport time\n\nprint(\"ğŸŒ™ GECE BOYU 3 STRATEJÄ° HAZIRLIÄI\")\nprint(\"=\"*70)\n\n# =================================================================\n# STRATEJÄ° 2: GEOGRAPHIC FEATURES Ä°LE Ä°YÄ°LEÅTÄ°RME\n# =================================================================\n\nprint(\"\\nğŸ—ºï¸ STRATEJÄ° 2: GEOGRAPHIC FEATURES\")\nprint(\"=\"*60)\n\ndef extract_geographic_features(address):\n    \"\"\"\n    Adresden coÄŸrafi Ã¶zellikleri Ã§Ä±kar\n    \"\"\"\n    if pd.isna(address):\n        return {}\n    \n    address = str(address).lower()\n    \n    features = {\n        'city': None,\n        'district': None,\n        'neighborhood': None,\n        'street_type': None,\n        'building_number': None\n    }\n    \n    # Åehir tespiti (yaygÄ±n ÅŸehirler)\n    cities = ['istanbul', 'ankara', 'izmir', 'bursa', 'antalya', 'adana', 'konya', \n              'gaziantep', 'kayseri', 'mersin', 'eskiÅŸehir', 'diyarbakÄ±r', 'samsun']\n    for city in cities:\n        if city in address:\n            features['city'] = city\n            break\n    \n    # Ä°lÃ§e tespiti (Ä°stanbul ilÃ§eleri)\n    districts = ['kadÄ±kÃ¶y', 'beÅŸiktaÅŸ', 'ÅŸiÅŸli', 'beyoÄŸlu', 'fatih', 'Ã¼skÃ¼dar', \n                'maltepe', 'pendik', 'kartal', 'ataÅŸehir', 'bakÄ±rkÃ¶y', 'zeytinburnu']\n    for district in districts:\n        if district in address:\n            features['district'] = district\n            break\n    \n    # Mahalle tespiti\n    if 'mahallesi' in address:\n        # \"X mahallesi\" pattern'ini bul\n        mah_match = re.search(r'(\\w+)\\s+mahallesi', address)\n        if mah_match:\n            features['neighborhood'] = mah_match.group(1)\n    \n    # Sokak tipi\n    if 'caddesi' in address:\n        features['street_type'] = 'caddesi'\n    elif 'sokak' in address:\n        features['street_type'] = 'sokak'\n    elif 'bulvari' in address:\n        features['street_type'] = 'bulvari'\n    \n    # Bina numarasÄ±\n    number_match = re.search(r'numara\\s*(\\d+)', address)\n    if number_match:\n        features['building_number'] = number_match.group(1)\n    \n    return features\n\ndef geographic_similarity(test_features, train_features):\n    \"\"\"\n    CoÄŸrafi Ã¶zelliklere gÃ¶re benzerlik hesapla\n    \"\"\"\n    score = 0.0\n    max_score = 0.0\n    \n    # Åehir eÅŸleÅŸmesi (en Ã¶nemli)\n    max_score += 0.4\n    if test_features['city'] and train_features['city']:\n        if test_features['city'] == train_features['city']:\n            score += 0.4\n    \n    # Ä°lÃ§e eÅŸleÅŸmesi\n    max_score += 0.3\n    if test_features['district'] and train_features['district']:\n        if test_features['district'] == train_features['district']:\n            score += 0.3\n    \n    # Mahalle eÅŸleÅŸmesi\n    max_score += 0.2\n    if test_features['neighborhood'] and train_features['neighborhood']:\n        if test_features['neighborhood'] == train_features['neighborhood']:\n            score += 0.2\n    \n    # Sokak tipi eÅŸleÅŸmesi\n    max_score += 0.1\n    if test_features['street_type'] and train_features['street_type']:\n        if test_features['street_type'] == train_features['street_type']:\n            score += 0.1\n    \n    return score / max_score if max_score > 0 else 0.0\n\ndef predict_with_geographic_features(test_df, train_df, batch_size=3000):\n    \"\"\"\n    CoÄŸrafi Ã¶zelliklerle tahmin\n    \"\"\"\n    print(\"ğŸ—ºï¸ CoÄŸrafi Ã¶zellik Ã§Ä±karÄ±mÄ± baÅŸlÄ±yor...\")\n    \n    # Train features Ã¶nceden hesapla\n    print(\"   Train coÄŸrafi Ã¶zellikleri hesaplanÄ±yor...\")\n    train_geo_features = {}\n    \n    for _, row in train_df.iterrows():\n        label = row['label']\n        address = row['address_normalized']\n        features = extract_geographic_features(address)\n        \n        if label not in train_geo_features:\n            train_geo_features[label] = []\n        train_geo_features[label].append(features)\n    \n    # Her label iÃ§in dominant Ã¶zellikleri bul\n    print(\"   Label temsilci Ã¶zellikleri oluÅŸturuluyor...\")\n    label_dominant_features = {}\n    \n    for label, feature_list in train_geo_features.items():\n        dominant = {\n            'city': Counter([f['city'] for f in feature_list if f['city']]).most_common(1),\n            'district': Counter([f['district'] for f in feature_list if f['district']]).most_common(1),\n            'neighborhood': Counter([f['neighborhood'] for f in feature_list if f['neighborhood']]).most_common(1),\n            'street_type': Counter([f['street_type'] for f in feature_list if f['street_type']]).most_common(1)\n        }\n        \n        # En yaygÄ±n Ã¶zellikleri al\n        label_dominant_features[label] = {\n            'city': dominant['city'][0][0] if dominant['city'] else None,\n            'district': dominant['district'][0][0] if dominant['district'] else None,\n            'neighborhood': dominant['neighborhood'][0][0] if dominant['neighborhood'] else None,\n            'street_type': dominant['street_type'][0][0] if dominant['street_type'] else None,\n        }\n    \n    print(f\"âœ… {len(label_dominant_features)} label iÃ§in coÄŸrafi Ã¶zellikler hazÄ±r\")\n    \n    # Test prediction\n    predictions = []\n    confidences = []\n    \n    total_batches = (len(test_df) + batch_size - 1) // batch_size\n    \n    for batch_idx in range(total_batches):\n        start_idx = batch_idx * batch_size\n        end_idx = min((batch_idx + 1) * batch_size, len(test_df))\n        \n        print(f\"   ğŸ“¦ Geo Batch {batch_idx + 1}/{total_batches}: {start_idx:,}-{end_idx:,}\")\n        \n        batch_df = test_df.iloc[start_idx:end_idx]\n        \n        for _, row in batch_df.iterrows():\n            test_address = row['address_normalized']\n            test_features = extract_geographic_features(test_address)\n            \n            best_score = 0.0\n            best_label = None\n            \n            # Her label ile karÅŸÄ±laÅŸtÄ±r\n            for label, train_features in label_dominant_features.items():\n                geo_score = geographic_similarity(test_features, train_features)\n                \n                if geo_score > best_score:\n                    best_score = geo_score\n                    best_label = label\n            \n            if best_label is None:\n                # Fallback - rastgele bir label\n                best_label = list(label_dominant_features.keys())[0]\n                best_score = 0.0\n            \n            predictions.append(best_label)\n            confidences.append(best_score)\n    \n    return predictions, confidences\n\n# =================================================================\n# STRATEJÄ° 3: ENSEMBLE METHODS (Hibrit YaklaÅŸÄ±m)\n# =================================================================\n\nprint(\"\\nğŸ¤ STRATEJÄ° 3: ENSEMBLE METHODS\")\nprint(\"=\"*60)\n\ndef weighted_ensemble_prediction(test_df, \n                                string_preds, string_confs,\n                                geo_preds, geo_confs,\n                                weights=[0.7, 0.3]):\n    \"\"\"\n    FarklÄ± yÃ¶ntemlerin aÄŸÄ±rlÄ±klÄ± kombinasyonu\n    \"\"\"\n    print(f\"ğŸ¤ Ensemble tahmin (weights: {weights})\")\n    \n    final_predictions = []\n    final_confidences = []\n    \n    for i in range(len(test_df)):\n        string_pred = string_preds[i]\n        string_conf = string_confs[i]\n        geo_pred = geo_preds[i]\n        geo_conf = geo_confs[i]\n        \n        # AÄŸÄ±rlÄ±klÄ± confidence hesapla\n        weighted_string = string_conf * weights[0]\n        weighted_geo = geo_conf * weights[1]\n        \n        # En yÃ¼ksek aÄŸÄ±rlÄ±klÄ± confidence'Ä± seÃ§\n        if weighted_string >= weighted_geo:\n            final_predictions.append(string_pred)\n            final_confidences.append(string_conf)\n        else:\n            final_predictions.append(geo_pred)\n            final_confidences.append(geo_conf)\n    \n    return final_predictions, final_confidences\n\ndef voting_ensemble_prediction(test_df,\n                              string_preds, string_confs,\n                              geo_preds, geo_confs,\n                              confidence_threshold=0.5):\n    \"\"\"\n    Voting tabanlÄ± ensemble\n    \"\"\"\n    print(f\"ğŸ—³ï¸ Voting ensemble (threshold: {confidence_threshold})\")\n    \n    final_predictions = []\n    final_confidences = []\n    \n    for i in range(len(test_df)):\n        string_pred = string_preds[i]\n        string_conf = string_confs[i]\n        geo_pred = geo_preds[i]\n        geo_conf = geo_confs[i]\n        \n        # YÃ¼ksek confidence'lÄ± tahminleri say\n        high_conf_votes = []\n        \n        if string_conf >= confidence_threshold:\n            high_conf_votes.append((string_pred, string_conf))\n        \n        if geo_conf >= confidence_threshold:\n            high_conf_votes.append((geo_pred, geo_conf))\n        \n        if high_conf_votes:\n            # En yÃ¼ksek confidence'lÄ± oyu seÃ§\n            best_vote = max(high_conf_votes, key=lambda x: x[1])\n            final_predictions.append(best_vote[0])\n            final_confidences.append(best_vote[1])\n        else:\n            # HiÃ§ yÃ¼ksek confidence yoksa string'i tercih et\n            final_predictions.append(string_pred)\n            final_confidences.append(string_conf)\n    \n    return final_predictions, final_confidences\n\n# =================================================================\n# OTOMATIK Ã‡ALIÅMA PIPELINE'I\n# =================================================================\n\nprint(\"\\nğŸ¤– OTOMATÄ°K Ã‡ALIÅMA PIPELINE'I HAZIR\")\nprint(\"=\"*70)\n\ndef run_all_strategies_pipeline():\n    \"\"\"\n    TÃ¼m stratejileri sÄ±rayla Ã§alÄ±ÅŸtÄ±r\n    \"\"\"\n    print(\"ğŸš€ PIPELINE BAÅLIYOR - GECE BOYU Ã‡ALIÅACAK\")\n    start_time = time.time()\n    \n    # Strateji 1: String Similarity (zaten Ã§alÄ±ÅŸÄ±yor - bekle)\n    print(\"\\n1ï¸âƒ£ String Similarity bekleniyor...\")\n    print(\"   (Manuel olarak tamamlanmasÄ±nÄ± bekle)\")\n    \n    # Strateji 2: Geographic Features\n    print(\"\\n2ï¸âƒ£ Geographic Features baÅŸlÄ±yor...\")\n    geo_start = time.time()\n    \n    geo_predictions, geo_confidences = predict_with_geographic_features(\n        test, train, batch_size=3000\n    )\n    \n    geo_time = time.time() - geo_start\n    print(f\"âœ… Geographic Features tamamlandÄ±: {geo_time/60:.1f} dakika\")\n    \n    # SonuÃ§larÄ± kaydet\n    test['geo_predicted_label'] = geo_predictions\n    test['geo_confidence'] = geo_confidences\n    \n    # Geographic submission\n    geo_submission = test[['id', 'geo_predicted_label']].copy()\n    geo_submission.columns = ['id', 'label']\n    geo_submission.to_csv('/kaggle/working/submission_geographic.csv', index=False)\n    print(\"ğŸ’¾ Geographic submission kaydedildi\")\n    \n    # Ä°statistikler\n    geo_conf_mean = np.mean(geo_confidences)\n    geo_high_conf = sum(1 for c in geo_confidences if c > 0.6)\n    \n    print(f\"ğŸ“Š Geographic confidence stats:\")\n    print(f\"   Ortalama: {geo_conf_mean:.3f}\")\n    print(f\"   YÃ¼ksek (>0.6): {geo_high_conf:,} (%{geo_high_conf/len(geo_confidences)*100:.1f})\")\n    \n    total_time = time.time() - start_time\n    print(f\"\\nâ° Toplam sÃ¼re: {total_time/60:.1f} dakika\")\n    print(\"\\nğŸ¯ PIPELINE HAZIR - STRING SIMILARITY BÄ°TÄ°NCE ENSEMBLE YAPILACAK\")\n\ndef run_ensemble_when_ready():\n    \"\"\"\n    String similarity bitince ensemble yap\n    \"\"\"\n    print(\"\\n3ï¸âƒ£ ENSEMBLE METHODS BAÅLIYOR...\")\n    \n    # String similarity sonuÃ§larÄ±nÄ± kontrol et\n    if 'predicted_label_v2' not in test.columns:\n        print(\"âŒ String similarity henÃ¼z tamamlanmamÄ±ÅŸ!\")\n        return\n    \n    # Ensemble predictions\n    print(\"ğŸ¤ Weighted Ensemble...\")\n    weighted_preds, weighted_confs = weighted_ensemble_prediction(\n        test,\n        test['predicted_label_v2'], test['confidence_v2'],\n        test['geo_predicted_label'], test['geo_confidence'],\n        weights=[0.7, 0.3]\n    )\n    \n    print(\"ğŸ—³ï¸ Voting Ensemble...\")\n    voting_preds, voting_confs = voting_ensemble_prediction(\n        test,\n        test['predicted_label_v2'], test['confidence_v2'],\n        test['geo_predicted_label'], test['geo_confidence'],\n        confidence_threshold=0.4\n    )\n    \n    # SonuÃ§larÄ± kaydet\n    test['weighted_ensemble_pred'] = weighted_preds\n    test['weighted_ensemble_conf'] = weighted_confs\n    test['voting_ensemble_pred'] = voting_preds\n    test['voting_ensemble_conf'] = voting_confs\n    \n    # Ensemble submissions\n    weighted_submission = test[['id', 'weighted_ensemble_pred']].copy()\n    weighted_submission.columns = ['id', 'label']\n    weighted_submission.to_csv('/kaggle/working/submission_weighted_ensemble.csv', index=False)\n    \n    voting_submission = test[['id', 'voting_ensemble_pred']].copy()\n    voting_submission.columns = ['id', 'label']\n    voting_submission.to_csv('/kaggle/working/submission_voting_ensemble.csv', index=False)\n    \n    print(\"ğŸ’¾ Ensemble submissions kaydedildi\")\n    \n    # Final stats\n    print(f\"\\nğŸ“Š FINAL CONFIDENCE KARÅILAÅTIRMASI:\")\n    print(f\"String Similarity: {np.mean(test['confidence_v2']):.3f}\")\n    print(f\"Geographic: {np.mean(test['geo_confidence']):.3f}\")\n    print(f\"Weighted Ensemble: {np.mean(weighted_confs):.3f}\")\n    print(f\"Voting Ensemble: {np.mean(voting_confs):.3f}\")\n\n# OTOMATIK BAÅLATMA\nprint(\"\\nğŸŒ™ GECE BOYUNCA Ã‡ALIÅMA BAÅLIYOR...\")\nprint(\"=\"*70)\nprint(\"1ï¸âƒ£ String Similarity devam ediyor...\")\nprint(\"2ï¸âƒ£ Geographic Features baÅŸlatÄ±lÄ±yor...\")\n\n# Geographic strategy'yi baÅŸlat\nrun_all_strategies_pipeline()\n\nprint(\"\\nğŸ¯ SABAH KALKTIKTA YAPILACAKLAR:\")\nprint(\"=\"*50)\nprint(\"1. String similarity tamamlandÄ±ysa kontrol et\")\nprint(\"2. AÅŸaÄŸÄ±daki kodu Ã§alÄ±ÅŸtÄ±r:\")\nprint()\nprint(\"# ENSEMBLE COMPLETION\")\nprint(\"run_ensemble_when_ready()\")\nprint()\nprint(\"3. Submission dosyalarÄ±nÄ± kontrol et:\")\nprint(\"   - submission_v2.csv (String)\")\nprint(\"   - submission_geographic.csv (Geographic)\")\nprint(\"   - submission_weighted_ensemble.csv (Ensemble)\")\nprint(\"   - submission_voting_ensemble.csv (Ensemble)\")\nprint()\nprint(\"4. En iyi performans gÃ¶steren submission'Ä± seÃ§!\")\nprint()\nprint(\"ğŸ˜´ Ä°yi uykular! Sistem gece boyu Ã§alÄ±ÅŸacak...\")\nprint(\"=\"*70)","metadata":{"_uuid":"78fa2926-4e59-4fb5-91d3-900b039c773f","_cell_guid":"29a2df0b-bf09-4ff4-973d-49b5c99b1b2c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-20T00:06:26.402968Z","iopub.execute_input":"2025-08-20T00:06:26.403388Z","execution_failed":"2025-08-20T11:27:56.221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test dataframe'inde hangi kolonlar var gÃ¶relim\nprint(\"Test DataFrame kolonlarÄ±:\")\nprint(test.columns.tolist())\nprint()\nprint(\"Test shape:\", test.shape)","metadata":{"_uuid":"9bf74dbb-447d-4799-a017-9f158723c070","_cell_guid":"55c1b3a4-a2e1-4b45-8c68-fcf556d15f9b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-08-20T11:27:56.223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Geographic vs TF-IDF ensemble\nweighted_preds, weighted_confs = weighted_ensemble_prediction(\n    test,\n    test['predicted_label'], test['confidence_score'],      # TF-IDF\n    test['geo_predicted_label'], test['geo_confidence'],    # Geographic\n    weights=[0.3, 0.7]  # Geographic'e daha Ã§ok aÄŸÄ±rlÄ±k ver\n)\n\n# Sonucu kaydet\ntest['ensemble_pred'] = weighted_preds\ntest['ensemble_conf'] = weighted_confs\n\nensemble_submission = test[['id', 'ensemble_pred']].copy()\nensemble_submission.columns = ['id', 'label']\nensemble_submission.to_csv('/kaggle/working/submission_ensemble_v1.csv', index=False)\nprint(\"âœ… Ensemble submission hazÄ±r!\")","metadata":{"_uuid":"8e6ab5d4-501a-4cd2-a845-c7c9b01d9640","_cell_guid":"97b076c2-f650-4294-94d9-a5804df60a83","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-08-20T11:27:56.224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# â† BURAYA YENÄ° KODU YAPIÅITIR:\nprint(\"\\nğŸ”§ Ensemble V2 - TF-IDF AÄŸÄ±rlÄ±klÄ±:\")\nweighted_preds, weighted_confs = weighted_ensemble_prediction(\n    test,\n    test['predicted_label'], test['confidence_score'],      # TF-IDF\n    test['geo_predicted_label'], test['geo_confidence'],    # Geographic  \n    weights=[0.8, 0.2]  # %80 TF-IDF, %20 Geographic\n)\n\n# Kaydet\ntest['ensemble_v2_pred'] = weighted_preds\nensemble_v2 = test[['id', 'ensemble_v2_pred']].copy()\nensemble_v2.columns = ['id', 'label']\nensemble_v2.to_csv('/kaggle/working/submission_ensemble_v2.csv', index=False)\nprint(\"âœ… Ensemble V2 hazÄ±r!\")","metadata":{"_uuid":"0d7e5edb-0a7e-4967-9a6f-4e492f6f91de","_cell_guid":"3a992ff2-5b95-4680-8bc0-3a1d25da30a9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-08-20T11:27:56.224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ENTEGRASYON REHBERÄ°: MEVCUt KOD + ADVANCED SYSTEM\n# Mevcut kodunuzun SONUNA ekleyin\n\nprint(\"ğŸ”„ ADVANCED SYSTEM ENTEGRASYONU\")\nprint(\"=\"*70)\n\n# =====================================================================\n# PHASE 1: MEVCUT VERÄ°LERÄ°NÄ°ZÄ° KULLANARAK ADVANCED SYSTEM\n# =====================================================================\n\n# Mevcut verilerinizi kullan (train, test zaten yÃ¼klÃ¼)\n# address_normalized kolonlarÄ± zaten var\n\nclass AdvancedAddressMatcher:\n    def __init__(self):\n        # Mevcut STANDARD_WORDS'Ä±nÄ±zÄ± geniÅŸlet\n        self.STANDARD_WORDS = {\n            'sokak': ['sok', 'sk', 's', 'sokagi', 'sokaÄŸÄ±', 'sokag', 'sokok', 'sokk', \n                     'skk', 'soak', 'skoak', 'sokakk', 'skak', 'soks', 'sokkak'],\n            'mahallesi': ['mah', 'mh', 'm', 'maha', 'mahl', 'mahle', 'mahall', \n                         'mahalle', 'mahal', 'mhal', 'mhle', 'mhall'],\n            'caddesi': ['cad', 'cd', 'c', 'cadde', 'caddesi', 'cde', 'cdd', 'cdde'],\n            'apartmani': ['apt', 'ap', 'apartman', 'apartmani', 'apartmn'],\n            'bulvari': ['bul', 'bulvar', 'bulvari', 'bv', 'blv', 'blvr'],\n            'numara': ['no', 'n', 'num', 'numara', 'nr', 'nmr'],\n            'daire': ['d', 'da', 'dr', 'daire', 'dair', 'dare']\n        }\n        \n        # Pattern'lar\n        self.NEIGHBORHOOD_PATTERN = r'(\\w+(?:\\s+\\w+)*?)\\s*(?:mahallesi|mah|mh|mahal|mahalle)\\b'\n        self.STREET_PATTERN = r'(\\w+(?:\\s+\\w+)*?)\\s*(?:sokak|sok|sk|caddesi|cad|cd|sokaÄŸÄ±|cadde|bulvari)\\b'\n        self.BUILDING_PATTERN = r'\\b(?:no:?\\s*)?(\\d+(?:[/-]\\d+)?)\\b'\n        \n        # Lookup tables\n        self.exact_fingerprints = {}\n        self.partial_matches = defaultdict(list)\n        self.cache = {}\n    \n    def normalize_word(self, word, threshold=85):\n        \"\"\"Mevcut normalize fonksiyonunuzla aynÄ± ama daha strict\"\"\"\n        if not word or len(word) < 2:\n            return word\n            \n        word = word.lower().strip()\n        \n        if word.isdigit():\n            return word\n            \n        if word in self.cache:\n            return self.cache[word]\n        \n        # Exact match\n        for standard, variants in self.STANDARD_WORDS.items():\n            if word in variants or word == standard:\n                self.cache[word] = standard\n                return standard\n        \n        # Fuzzy match (threshold artÄ±rÄ±ldÄ±)\n        best_match = word\n        best_score = 0\n        \n        for standard, variants in self.STANDARD_WORDS.items():\n            for variant in variants:\n                if abs(len(word) - len(variant)) > 2:\n                    continue\n                \n                score = fuzz.ratio(word, variant)\n                if score > best_score and score >= threshold:\n                    best_score = score\n                    best_match = standard\n        \n        self.cache[word] = best_match\n        return best_match\n    \n    def extract_components(self, address):\n        \"\"\"Adresi componentlere ayÄ±r\"\"\"\n        if not address:\n            return {'neighborhood': '', 'street': '', 'building': ''}\n        \n        address = str(address).lower()\n        \n        components = {}\n        \n        # Mahalle\n        match = re.search(self.NEIGHBORHOOD_PATTERN, address)\n        components['neighborhood'] = self.normalize_word(match.group(1)) if match else ''\n        \n        # Sokak\n        match = re.search(self.STREET_PATTERN, address)\n        components['street'] = self.normalize_word(match.group(1)) if match else ''\n        \n        # Bina no\n        match = re.search(self.BUILDING_PATTERN, address)\n        components['building'] = match.group(1) if match else ''\n        \n        return components\n    \n    def create_fingerprint(self, address):\n        \"\"\"Address fingerprint oluÅŸtur\"\"\"\n        components = self.extract_components(address)\n        fingerprint = f\"{components['neighborhood']}|{components['street']}|{components['building']}\"\n        return fingerprint.replace('||', '|').strip('|')\n    \n    def build_database(self, train_df):\n        \"\"\"Train verisiyle database oluÅŸtur\"\"\"\n        print(\"ğŸ—ï¸ Advanced database oluÅŸturuluyor...\")\n        \n        for _, row in train_df.iterrows():\n            address = row['address_normalized']\n            label = row['label']\n            \n            fingerprint = self.create_fingerprint(address)\n            components = self.extract_components(address)\n            \n            # Exact match table\n            if fingerprint not in self.exact_fingerprints:\n                self.exact_fingerprints[fingerprint] = []\n            self.exact_fingerprints[fingerprint].append(label)\n            \n            # Partial match table\n            if components['neighborhood'] and components['street']:\n                partial_key = f\"{components['neighborhood']}|{components['street']}\"\n                self.partial_matches[partial_key].append((label, components['building']))\n        \n        # En yaygÄ±n label'larÄ± seÃ§\n        for fp in self.exact_fingerprints:\n            labels = self.exact_fingerprints[fp]\n            most_common = Counter(labels).most_common(1)[0][0]\n            self.exact_fingerprints[fp] = most_common\n        \n        print(f\"âœ… {len(self.exact_fingerprints)} exact fingerprint hazÄ±r\")\n        print(f\"âœ… {len(self.partial_matches)} partial match hazÄ±r\")\n    \n    def predict_single(self, address):\n        \"\"\"Tek adres iÃ§in geliÅŸmiÅŸ tahmin\"\"\"\n        fingerprint = self.create_fingerprint(address)\n        components = self.extract_components(address)\n        \n        # Level 1: Exact match\n        if fingerprint in self.exact_fingerprints:\n            return self.exact_fingerprints[fingerprint], 0.95, \"exact\"\n        \n        # Level 2: Partial match (mahalle + sokak)\n        if components['neighborhood'] and components['street']:\n            partial_key = f\"{components['neighborhood']}|{components['street']}\"\n            \n            if partial_key in self.partial_matches:\n                candidates = self.partial_matches[partial_key]\n                \n                # Bina numarasÄ± varsa exact match dene\n                if components['building']:\n                    for label, building in candidates:\n                        if building == components['building']:\n                            return label, 0.85, \"partial_exact\"\n                \n                # En yaygÄ±n label'Ä± al\n                labels = [label for label, _ in candidates]\n                most_common = Counter(labels).most_common(1)[0][0]\n                return most_common, 0.75, \"partial_common\"\n        \n        # Level 3: Fuzzy mahalle match\n        if components['neighborhood']:\n            for partial_key in self.partial_matches:\n                key_neighborhood = partial_key.split('|')[0]\n                similarity = fuzz.ratio(components['neighborhood'], key_neighborhood)\n                \n                if similarity >= 85:  # YÃ¼ksek threshold\n                    candidates = self.partial_matches[partial_key]\n                    labels = [label for label, _ in candidates]\n                    most_common = Counter(labels).most_common(1)[0][0]\n                    return most_common, 0.6, \"fuzzy\"\n        \n        # Level 4: Fallback - mevcut TF-IDF sonucunu kullan\n        return None, 0.0, \"fallback\"\n    \n    def predict_batch(self, test_addresses, batch_size=5000):\n        \"\"\"Batch prediction\"\"\"\n        predictions = []\n        confidences = []\n        match_types = []\n        \n        print(f\"ğŸ”® Advanced prediction iÃ§in {len(test_addresses):,} adres iÅŸleniyor...\")\n        \n        for i in range(0, len(test_addresses), batch_size):\n            batch = test_addresses[i:i+batch_size]\n            batch_num = i // batch_size + 1\n            total_batches = (len(test_addresses) + batch_size - 1) // batch_size\n            \n            print(f\"   ğŸ“¦ Batch {batch_num}/{total_batches}\")\n            \n            for address in batch:\n                pred, conf, match_type = self.predict_single(address)\n                predictions.append(pred)\n                confidences.append(conf)\n                match_types.append(match_type)\n        \n        return predictions, confidences, match_types\n\n# =====================================================================\n# PHASE 2: SMART ENSEMBLE (MEVCUT + ADVANCED)\n# =====================================================================\n\ndef smart_ensemble_with_existing(test_df, \n                                existing_preds, existing_confs,\n                                advanced_preds, advanced_confs, advanced_types):\n    \"\"\"Mevcut TF-IDF + Advanced sistem ensemble\"\"\"\n    \n    print(\"ğŸ¤ Smart ensemble: Mevcut TF-IDF + Advanced system\")\n    \n    final_predictions = []\n    final_confidences = []\n    \n    for i in range(len(test_df)):\n        existing_pred = existing_preds[i]\n        existing_conf = existing_confs[i]\n        \n        advanced_pred = advanced_preds[i] \n        advanced_conf = advanced_confs[i]\n        advanced_type = advanced_types[i]\n        \n        # Decision logic\n        if advanced_type == \"exact\":\n            # Exact match'e gÃ¼ven\n            final_predictions.append(advanced_pred)\n            final_confidences.append(advanced_conf)\n        elif advanced_type == \"partial_exact\":\n            # Partial exact'e gÃ¼ven\n            final_predictions.append(advanced_pred)\n            final_confidences.append(advanced_conf)\n        elif advanced_type == \"partial_common\":\n            # Confidence'a gÃ¶re karar ver\n            if advanced_conf > existing_conf:\n                final_predictions.append(advanced_pred)\n                final_confidences.append(advanced_conf)\n            else:\n                final_predictions.append(existing_pred)\n                final_confidences.append(existing_conf)\n        elif advanced_type == \"fuzzy\":\n            # Mevcut TF-IDF'e gÃ¼ven (semantic similarity iÃ§in)\n            final_predictions.append(existing_pred)\n            final_confidences.append(existing_conf)\n        else:  # fallback\n            # Mevcut TF-IDF sonucu kullan\n            final_predictions.append(existing_pred)\n            final_confidences.append(existing_conf)\n    \n    return final_predictions, final_confidences\n\n# =====================================================================\n# PHASE 3: EXECUTION - MEVCUt KODUNUZDAKÄ° VERÄ°LERÄ° KULLAN\n# =====================================================================\n\ndef run_advanced_on_existing_data():\n    \"\"\"Mevcut verileriniz Ã¼zerinde advanced sistemi Ã§alÄ±ÅŸtÄ±r\"\"\"\n    \n    print(\"\\nğŸš€ ADVANCED SYSTEM - MEVCUt VERÄ°LER ÃœZERÄ°NDE\")\n    print(\"=\"*70)\n    \n    # Mevcut verilerinizi kontrol et\n    if 'predicted_label' not in test.columns:\n        print(\"âŒ Mevcut TF-IDF tahminleri bulunamadÄ±!\")\n        print(\"Ã–nce mevcut kodunuzu Ã§alÄ±ÅŸtÄ±rÄ±n, sonra bu sistemi Ã§alÄ±ÅŸtÄ±rÄ±n\")\n        return\n    \n    # Advanced matcher oluÅŸtur\n    advanced_matcher = AdvancedAddressMatcher()\n    \n    # Database oluÅŸtur\n    advanced_matcher.build_database(train)\n    \n    # Advanced predictions\n    test_addresses = test['address_normalized'].tolist()\n    advanced_preds, advanced_confs, advanced_types = advanced_matcher.predict_batch(test_addresses)\n    \n    # Mevcut sonuÃ§larla ensemble\n    existing_preds = test['predicted_label'].tolist()\n    existing_confs = test['confidence_score'].tolist()\n    \n    final_preds, final_confs = smart_ensemble_with_existing(\n        test, existing_preds, existing_confs,\n        advanced_preds, advanced_confs, advanced_types\n    )\n    \n    # SonuÃ§larÄ± ekle\n    test['advanced_prediction'] = advanced_preds\n    test['advanced_confidence'] = advanced_confs\n    test['advanced_match_type'] = advanced_types\n    test['final_ensemble_prediction'] = final_preds\n    test['final_ensemble_confidence'] = final_confs\n    \n    # Performance analizi\n    print(f\"\\nğŸ“Š ADVANCED SYSTEM PERFORMANCE:\")\n    print(\"=\"*50)\n    \n    match_type_stats = Counter(advanced_types)\n    for match_type, count in match_type_stats.items():\n        pct = count / len(advanced_types) * 100\n        print(f\"{match_type}: {count:,} ({pct:.1f}%)\")\n    \n    # Confidence karÅŸÄ±laÅŸtÄ±rmasÄ±\n    print(f\"\\nğŸ“ˆ CONFIDENCE KARÅILAÅTIRMASI:\")\n    print(f\"Mevcut TF-IDF ortalama: {np.mean(existing_confs):.3f}\")\n    print(f\"Advanced system ortalama: {np.mean(advanced_confs):.3f}\")\n    print(f\"Final ensemble ortalama: {np.mean(final_confs):.3f}\")\n    \n    # High confidence oranlarÄ±\n    existing_high = sum(1 for c in existing_confs if c > 0.7)\n    advanced_high = sum(1 for c in advanced_confs if c > 0.7)\n    final_high = sum(1 for c in final_confs if c > 0.7)\n    \n    print(f\"\\nYÃ¼ksek confidence (>0.7) oranlarÄ±:\")\n    print(f\"Mevcut: {existing_high:,} ({existing_high/len(existing_confs)*100:.1f}%)\")\n    print(f\"Advanced: {advanced_high:,} ({advanced_high/len(advanced_confs)*100:.1f}%)\")\n    print(f\"Final: {final_high:,} ({final_high/len(final_confs)*100:.1f}%)\")\n    \n    # Submission dosyalarÄ± oluÅŸtur\n    submissions = [\n        ('submission_advanced_only.csv', 'advanced_prediction'),\n        ('submission_final_ensemble.csv', 'final_ensemble_prediction')\n    ]\n    \n    for filename, column in submissions:\n        submission = test[['id', column]].copy()\n        submission.columns = ['id', 'label']\n        submission.to_csv(f'/kaggle/working/{filename}', index=False)\n        print(f\"ğŸ’¾ {filename} kaydedildi\")\n    \n    print(f\"\\nâœ… ADVANCED SYSTEM TAMAMLANDI!\")\n    print(\"=\"*70)\n    print(\"ğŸ¯ EXPECTED IMPROVEMENT:\")\n    print(\"â€¢ Exact + Partial matches: BÃ¼yÃ¼k doÄŸruluk artÄ±ÅŸÄ±\")\n    print(\"â€¢ Smart ensemble: En iyi iki sistemin kombinasyonu\") \n    print(\"â€¢ Beklenen doÄŸruluk: %40-70 (mevcut %14'ten)\")\n    print(\"\\nğŸš€ Test etmek iÃ§in submission_final_ensemble.csv'yi kullanÄ±n!\")\n\n# =====================================================================\n# Ã‡ALIÅTIR\n# =====================================================================\n\n# Bu fonksiyonu Ã§alÄ±ÅŸtÄ±rÄ±n\nrun_advanced_on_existing_data()","metadata":{"_uuid":"3224dfe8-518c-4f82-8b4a-71877b3d412b","_cell_guid":"7e11399a-41a1-4cb1-85c9-14e03a2c728e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-08-20T11:27:56.224Z"}},"outputs":[],"execution_count":null}]}