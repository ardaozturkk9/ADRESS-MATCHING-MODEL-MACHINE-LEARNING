{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12801953,"sourceType":"datasetVersion","datasetId":8094292},{"sourceId":12810518,"sourceType":"datasetVersion","datasetId":8100430}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install fuzzywuzzy\n!pip install python-levenshtein\n!pip install unidecode\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"1c7784eb-1d71-4450-9968-95520393ecf3","_cell_guid":"b0651aff-2422-4a84-9147-9c3756f1f1a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-19T23:57:38.415938Z","iopub.execute_input":"2025-08-19T23:57:38.416313Z","iopub.status.idle":"2025-08-19T23:57:57.851598Z","shell.execute_reply.started":"2025-08-19T23:57:38.416286Z","shell.execute_reply":"2025-08-19T23:57:57.850348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# KAGGLE INPUT DEBUG - TAM PATH KONTROLÜ\nimport os\nimport pandas as pd\n\nprint(\"🔍 KAGGLE INPUT FULL DEBUG\")\nprint(\"=\"*60)\n\n# 1. Input dizininin tamamını listele\nprint(\"📁 /kaggle/input içeriği:\")\ntry:\n    for item in os.listdir('/kaggle/input'):\n        item_path = f'/kaggle/input/{item}'\n        if os.path.isdir(item_path):\n            print(f\"📂 {item}/\")\n            # Alt dizin içeriğini göster\n            try:\n                for sub_item in os.listdir(item_path):\n                    print(f\"   📄 {sub_item}\")\n            except:\n                print(f\"   ❌ Alt dizin okunamadı\")\n        else:\n            print(f\"📄 {item}\")\nexcept Exception as e:\n    print(f\"❌ /kaggle/input okunamadı: {e}\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# 2. Spesifik path'leri test et\ntest_paths = [\n    '/kaggle/input/datalar/train.csv',\n    '/kaggle/input/datalar/test.csv', \n    '/kaggle/input/datalar/sample_submission.csv'\n]\n\nprint(\"🎯 PATH TEST SONUÇLARI:\")\nprint(\"-\"*40)\n\nfor path in test_paths:\n    print(f\"\\n📍 Test ediliyor: {path}\")\n    \n    # File existence check\n    if os.path.exists(path):\n        print(\"✅ Dosya var\")\n        \n        # File readable check\n        try:\n            with open(path, 'r') as f:\n                first_line = f.readline()\n            print(f\"✅ Okunabilir - İlk satır: {first_line[:50]}...\")\n            \n            # Pandas read check\n            try:\n                df = pd.read_csv(path, nrows=1)  # Sadece 1 satır oku\n                print(f\"✅ Pandas ile okunabilir - Shape: {df.shape}\")\n                print(f\"✅ Kolonlar: {list(df.columns)}\")\n            except Exception as e:\n                print(f\"❌ Pandas hatası: {e}\")\n                \n        except Exception as e:\n            print(f\"❌ Dosya okuma hatası: {e}\")\n    else:\n        print(\"❌ Dosya bulunamadı\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# 3. Alternatif path bulma\nprint(\"🔍 TÜM CSV DOSYALARINI ARAMA:\")\nprint(\"-\"*40)\n\nfound_csv = []\nfor root, dirs, files in os.walk('/kaggle/input'):\n    for file in files:\n        if file.endswith('.csv'):\n            full_path = os.path.join(root, file)\n            found_csv.append((file, full_path))\n\nif found_csv:\n    print(\"📋 Bulunan CSV dosyaları:\")\n    for filename, fullpath in found_csv:\n        print(f\"  {filename} → {fullpath}\")\nelse:\n    print(\"❌ Hiç CSV dosyası bulunamadı!\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# 4. WORKING SOLUTION\nprint(\"💡 ÇALIŞAN ÇÖZÜM:\")\nprint(\"-\"*30)\n\n# En güvenli path'i bul\nworking_paths = {}\nfor filename, fullpath in found_csv:\n    if 'train' in filename.lower():\n        working_paths['train'] = fullpath\n    elif 'test' in filename.lower() and 'sample' not in filename.lower():\n        working_paths['test'] = fullpath  \n    elif 'sample' in filename.lower() or 'submission' in filename.lower():\n        working_paths['sample'] = fullpath\n\nif working_paths:\n    print(\"✅ KULLANILACAK PATH'LER:\")\n    for key, path in working_paths.items():\n        print(f\"{key} = pd.read_csv('{path}')\")\n    \n    print(f\"\\n🚀 TEST LOADING:\")\n    try:\n        if 'train' in working_paths:\n            train = pd.read_csv(working_paths['train'])\n            print(f\"✅ train yüklendi: {train.shape}\")\n        \n        if 'test' in working_paths:\n            test = pd.read_csv(working_paths['test'])\n            print(f\"✅ test yüklendi: {test.shape}\")\n            \n        if 'sample' in working_paths:\n            sample = pd.read_csv(working_paths['sample'])\n            print(f\"✅ sample yüklendi: {sample.shape}\")\n            \n        print(\"\\n🎉 TÜM DOSYALAR BAŞARIYLA YÜKLENDİ!\")\n        print(\"Artık kodunuza devam edebilirsiniz!\")\n        \n    except Exception as e:\n        print(f\"❌ Yükleme hatası: {e}\")\n        \nelse:\n    print(\"❌ Uygun dosyalar bulunamadı!\")\n\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T23:57:57.853577Z","iopub.execute_input":"2025-08-19T23:57:57.854541Z","iopub.status.idle":"2025-08-19T23:58:01.926704Z","shell.execute_reply.started":"2025-08-19T23:57:57.854470Z","shell.execute_reply":"2025-08-19T23:58:01.925397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bu satırı kodunuzun en başına ekleyin\n!pip install unidecode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T23:58:01.927824Z","iopub.execute_input":"2025-08-19T23:58:01.928139Z","iopub.status.idle":"2025-08-19T23:58:06.435693Z","shell.execute_reply.started":"2025-08-19T23:58:01.928111Z","shell.execute_reply":"2025-08-19T23:58:06.434539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ADIM 2: KARAKTER DÜZELTMİ VE ENCODİNG TEMİZLİĞİ\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom unidecode import unidecode\n\n# Verileri yükle (ADIM 1'den devam)\n# Path'lerini değiştir!\n# KAGGLE İÇİN DOĞRU PATH'LER\ntrain = pd.read_csv('/kaggle/input/datalar/train.csv')\ntest = pd.read_csv('/kaggle/input/datalar/test.csv')\nsample = pd.read_csv('/kaggle/input/datalar/sample_submission.csv')\n\n\nprint(\"🔧 ADIM 2: KARAKTER DÜZELTMİ\")\nprint(\"=\"*60)\n\n# Problematik karakterleri tanımla\nENCODING_FIXES = {\n    # Türkçe karakter bozuklukları\n    'Ä±': 'ı', 'Ä°': 'İ', 'Ä': 'İ',\n    'Ã±': 'ñ', 'Ã¼': 'ü', 'Ã¶': 'ö', \n    'Ã§': 'ç', 'Ã': 'ş', 'Ä': 'ğ',\n    '±': 'ı', 'Ã¥': 'å', 'Ã¤': 'ä',\n    'â': 'a', 'Ãª': 'e', 'Ã´': 'o',\n    \n    # HTML entities\n    '&amp;': '&', '&lt;': '<', '&gt;': '>',\n    '&quot;': '\"', '&#39;': \"'\",\n    \n    # Diğer encoding problemleri\n    'â€™': \"'\", 'â€œ': '\"', 'â€': '\"',\n    'â€\"': '-', 'â€\"': '-',\n}\n\ndef fix_encoding_issues(text):\n    \"\"\"Encoding problemlerini düzelt\"\"\"\n    if pd.isna(text):\n        return text\n    \n    text = str(text)\n    \n    # Encoding düzeltmeleri uygula\n    for wrong, correct in ENCODING_FIXES.items():\n        text = text.replace(wrong, correct)\n    \n    return text\n\ndef clean_turkish_text(text):\n    \"\"\"Türkçe metin temizliği\"\"\"\n    if pd.isna(text):\n        return text\n    \n    text = str(text)\n    \n    # Encoding düzeltmeleri\n    text = fix_encoding_issues(text)\n    \n    # Fazla boşlukları temizle\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Başta ve sonda boşluk temizle\n    text = text.strip()\n    \n    # Birden fazla noktalama işaretini tek yap\n    text = re.sub(r'\\.{2,}', '.', text)\n    text = re.sub(r',{2,}', ',', text)\n    text = re.sub(r':{2,}', ':', text)\n    \n    return text\n\n# ÖNCESİ - Problemli karakter sayıları\nprint(\"🔍 PROBLEM TESPİTİ (ÖNCESİ)\")\nprint(\"=\"*40)\n\nproblem_chars = ['Ä±', 'Ä°', 'Ã±', 'Ã¼', '±', 'Ã§', 'Ã¶']\n\nprint(\"TRAIN VERİSİ:\")\ntrain_problems_before = {}\nfor char in problem_chars:\n    count = train['address'].str.contains(char, regex=False, na=False).sum()\n    train_problems_before[char] = count\n    if count > 0:\n        print(f\"  '{char}' içeren: {count:,}\")\n\nprint(\"\\nTEST VERİSİ:\")\ntest_problems_before = {}\nfor char in problem_chars:\n    count = test['address'].str.contains(char, regex=False, na=False).sum()\n    test_problems_before[char] = count\n    if count > 0:\n        print(f\"  '{char}' içeren: {count:,}\")\n\n# Örnek problemli adresler göster\nprint(\"\\n📝 PROBLEMLİ ADRES ÖRNEKLERİ:\")\nprint(\"=\"*40)\n\nfor char in ['Ä±', 'Ä°', '±']:\n    problematic = train[train['address'].str.contains(char, regex=False, na=False)]['address'].head(3)\n    if len(problematic) > 0:\n        print(f\"\\n'{char}' içeren örnekler:\")\n        for i, addr in enumerate(problematic, 1):\n            print(f\"  {i}. {addr[:80]}{'...' if len(addr) > 80 else ''}\")\n\n# ADRES TEMİZLEME İŞLEMİ\nprint(\"\\n🧹 ADRES TEMİZLEME İŞLEMİ BAŞLIYOR...\")\nprint(\"=\"*50)\n\n# Train verisi temizliği\nprint(\"Train adresleri temizleniyor...\")\ntrain['address_cleaned'] = train['address'].apply(clean_turkish_text)\n\n# Test verisi temizliği  \nprint(\"Test adresleri temizleniyor...\")\ntest['address_cleaned'] = test['address'].apply(clean_turkish_text)\n\nprint(\"✅ Temizleme tamamlandı!\")\n\n# SONRASI - Problemli karakter sayıları\nprint(\"\\n🎯 SONUÇLAR (SONRASI)\")\nprint(\"=\"*40)\n\nprint(\"TRAIN VERİSİ:\")\ntrain_problems_after = {}\nfor char in problem_chars:\n    count = train['address_cleaned'].str.contains(char, regex=False, na=False).sum()\n    train_problems_after[char] = count\n    before = train_problems_before[char]\n    improvement = before - count\n    if before > 0:\n        print(f\"  '{char}': {before:,} → {count:,} (düzeltilen: {improvement:,})\")\n\nprint(\"\\nTEST VERİSİ:\")\ntest_problems_after = {}\nfor char in problem_chars:\n    count = test['address_cleaned'].str.contains(char, regex=False, na=False).sum()\n    test_problems_after[char] = count\n    before = test_problems_before[char]\n    improvement = before - count\n    if before > 0:\n        print(f\"  '{char}': {before:,} → {count:,} (düzeltilen: {improvement:,})\")\n\n# ÖRNEKLERİ KARŞILAŞTIR\nprint(\"\\n📊 ÖNCE vs SONRA ÖRNEKLERİ:\")\nprint(\"=\"*50)\n\n# Rastgele 5 adres karşılaştırması\nsample_indices = train.sample(5, random_state=42).index\n\nprint(\"ÖNCE → SONRA:\")\nfor i, idx in enumerate(sample_indices, 1):\n    original = train.loc[idx, 'address']\n    cleaned = train.loc[idx, 'address_cleaned']\n    print(f\"\\n{i}. ÖNCE:  {original[:70]}{'...' if len(original) > 70 else ''}\")\n    print(f\"   SONRA: {cleaned[:70]}{'...' if len(cleaned) > 70 else ''}\")\n    if original != cleaned:\n        print(f\"   ✅ DEĞİŞTİ\")\n    else:\n        print(f\"   → Değişiklik yok\")\n\n# İSTATİSTİKLER\nprint(\"\\n📈 TEMİZLEME İSTATİSTİKLERİ:\")\nprint(\"=\"*50)\n\n# Kaç adres değişti?\ntrain_changed = (train['address'] != train['address_cleaned']).sum()\ntest_changed = (test['address'] != test['address_cleaned']).sum()\n\nprint(f\"Train'de değişen adres sayısı: {train_changed:,} / {len(train):,} (%{train_changed/len(train)*100:.1f})\")\nprint(f\"Test'te değişen adres sayısı: {test_changed:,} / {len(test):,} (%{test_changed/len(test)*100:.1f})\")\n\n# Ortalama uzunluk değişimi\ntrain_length_diff = train['address_cleaned'].str.len() - train['address'].str.len()\ntest_length_diff = test['address_cleaned'].str.len() - test['address'].str.len()\n\nprint(f\"\\nOrtalama uzunluk değişimi:\")\nprint(f\"Train: {train_length_diff.mean():.2f} karakter\")\nprint(f\"Test: {test_length_diff.mean():.2f} karakter\")\n\n# EK TEMİZLEME KONTROLLERI\nprint(\"\\n🔍 EK KONTROLLER:\")\nprint(\"=\"*30)\n\n# Null değerler kontrol\nprint(f\"Train'de null adres: {train['address_cleaned'].isna().sum()}\")\nprint(f\"Test'te null adres: {test['address_cleaned'].isna().sum()}\")\n\n# Çok kısa adresler\nshort_train = (train['address_cleaned'].str.len() < 5).sum()\nshort_test = (test['address_cleaned'].str.len() < 5).sum()\nprint(f\"Çok kısa adresler (< 5 karakter): Train={short_train}, Test={short_test}\")\n\n# Çok uzun adresler  \nlong_train = (train['address_cleaned'].str.len() > 200).sum()\nlong_test = (test['address_cleaned'].str.len() > 200).sum()\nprint(f\"Çok uzun adresler (> 200 karakter): Train={long_train}, Test={long_test}\")\n\nprint(\"\\n✅ ADIM 2 TAMAMLANDI!\")\nprint(\"=\"*60)\nprint(\"🎯 BAŞARILAR:\")\nprint(\"• Encoding sorunları düzeltildi\")\nprint(\"• Türkçe karakterler standardize edildi\") \nprint(\"• Fazla boşluklar temizlendi\")\nprint(\"• Noktalama düzeltmeleri yapıldı\")\nprint()\nprint(\"📁 YENİ KOLONLAR:\")\nprint(\"• train['address_cleaned'] - Temizlenmiş train adresleri\")\nprint(\"• test['address_cleaned'] - Temizlenmiş test adresleri\") \nprint()\nprint(\"🎯 SIRADA: ADIM 3 - Adres Temizleme ve Kısaltma Genişletme\")\nprint(\"=\"*60)","metadata":{"_uuid":"c64bea68-0934-41c9-912b-b54db5aaad1d","_cell_guid":"fe1a616f-b929-4b77-97c8-b08ced676424","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-19T23:58:06.438803Z","iopub.execute_input":"2025-08-19T23:58:06.439125Z","iopub.status.idle":"2025-08-19T23:58:36.335536Z","shell.execute_reply.started":"2025-08-19T23:58:06.439097Z","shell.execute_reply":"2025-08-19T23:58:36.333832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ADIM 3 - KAGGLE İÇİN FUZZY MATCHING + BATCH (HATASIZ)\n# Önce gerekli kütüphaneleri yükle ve import et\n\n\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom fuzzywuzzy import fuzz, process\nfrom collections import Counter\nimport time\nimport gc\n\nprint(\"🎯 ADIM 3: KAGGLE İÇİN FUZZY MATCHING + BATCH\")\nprint(\"=\"*70)\n\n# VERİ YÜKLEME (Kaggle için path'ler)\nprint(\"📂 Veriler yükleniyor...\")\n# KAGGLE İÇİN DOĞRU PATH'LER\n\ntry:\n    train = pd.read_csv('/kaggle/input/hackathon-data/train.csv')\n    test = pd.read_csv('/kaggle/input/hackathon-data/test.csv')\n    sample = pd.read_csv('/kaggle/input/hackathon-data/sample_submission.csv')\n    print(\"✅ Veriler başarıyla yüklendi!\")\nexcept:\n    print(\"⚠️ Kaggle input path'i bulunamadı. Manuel path kullanın:\")\n    print(\"train = pd.read_csv('your_path/train.csv')\")\n    print(\"test = pd.read_csv('your_path/test.csv')\")\n\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\n\n# ENCODING DÜZELTMESİ (ADIM 2'den)\nENCODING_FIXES = {\n    'Ä±': 'ı', 'Ä°': 'İ', 'Ä': 'İ',\n    'Ã±': 'ñ', 'Ã¼': 'ü', 'Ã¶': 'ö', \n    'Ã§': 'ç', 'Ã': 'ş', 'Ä': 'ğ',\n    '±': 'ı', 'â': 'a', '&amp;': '&',\n}\n\ndef fix_encoding(text):\n    \"\"\"Encoding problemlerini düzelt\"\"\"\n    if pd.isna(text):\n        return text\n    text = str(text)\n    for wrong, correct in ENCODING_FIXES.items():\n        text = text.replace(wrong, correct)\n    return re.sub(r'\\s+', ' ', text).strip()\n\n# Eğer address_cleaned kolonu yoksa oluştur\nif 'address_cleaned' not in train.columns:\n    print(\"🔧 Encoding düzeltmesi yapılıyor...\")\n    train['address_cleaned'] = train['address'].apply(fix_encoding)\n    test['address_cleaned'] = test['address'].apply(fix_encoding)\n    print(\"✅ Encoding düzeltmesi tamamlandı!\")\n\n# FUZZY MATCHING İÇİN STANDART KELİMELER\nSTANDARD_WORDS = {\n    'sokak': ['sok', 'sk', 's', 'sokagi', 'sokag', 'sokok', 'sokk', 'skk', 'soak', 'skoak', \n              'sokakk', 'skak', 'soks', 'sokkak', 'sookak', 'sokaak', 'sokag', 'sokakk'],\n    \n    'mahallesi': ['mah', 'mh', 'm', 'maha', 'mahl', 'mahle', 'mahall', 'mahalle', 'mahal',\n                  'mhal', 'mhle', 'mhall', 'mhallesi', 'mahles', 'mahale', 'mahallsi', 'mahallesi'],\n    \n    'caddesi': ['cad', 'cd', 'c', 'cadde', 'caddesi', 'cde', 'cdd', 'cdde', 'caddde', \n                'cadesi', 'cdes', 'cdsi', 'kadde', 'kaddesi', 'caddesi'],\n    \n    'numara': ['no', 'n', 'num', 'numara', 'nr', 'nmr', 'numr', 'numars', 'nummara',\n               'numero', 'numar', 'numera', 'numara'],\n    \n    'apartmani': ['apt', 'ap', 'apartman', 'apartmani', 'apartmn', 'aprt', 'aprtmn',\n                  'aprtn', 'aprtman', 'apartmaan', 'apartmani'],\n    \n    'blok': ['bl', 'b', 'blok', 'blk', 'blokk', 'block', 'blook', 'blok'],\n    \n    'daire': ['d', 'da', 'dr', 'daire', 'dair', 'dare', 'daira', 'daires', 'dairre', 'daire'],\n    \n    'kat': ['k', 'kt', 'kat', 'katt', 'kaat', 'kate', 'kat'],\n    \n    'sitesi': ['sit', 'site', 'sitesi', 'sites', 'siteesi', 'siitesi', 'sitesi'],\n    \n    'bulvari': ['bul', 'bulvar', 'bulvari', 'bv', 'blv', 'blvr', 'bulvr', 'bulvaari', 'bulvari'],\n    \n    'yolu': ['yl', 'yol', 'yolu', 'yoll', 'yoolu', 'yolu'],\n    \n    'plaza': ['plz', 'pl', 'plaza', 'plaza'],\n    \n    'hastanesi': ['hast', 'hastan', 'hastane', 'hastanesi', 'hastanesi'],\n    \n    'okulu': ['ok', 'okul', 'okulu', 'okulu'],\n    \n    'universitesi': ['univ', 'univers', 'universite', 'universitesi', 'üniv', 'üniver', \n                     'üniversite', 'üniversitesi', 'universitesi'],\n    \n    'merkezi': ['mrk', 'merkez', 'merkezi', 'merkezi'],\n}\n\n# PERFORMANS CACHE SİSTEMİ\nword_cache = {}\ncache_hits = 0\ncache_misses = 0\n\ndef fuzzy_standardize_word(word, threshold=75):\n    \"\"\"\n    Cache'li fuzzy matching ile kelime standardizasyonu\n    \"\"\"\n    global cache_hits, cache_misses\n    \n    if not word or len(word) < 2:\n        return word\n    \n    word = word.lower().strip()\n    \n    # Sayı ise dokunma\n    if word.isdigit():\n        return word\n    \n    # Cache kontrolü\n    if word in word_cache:\n        cache_hits += 1\n        return word_cache[word]\n    \n    cache_misses += 1\n    \n    # Exact match (hızlı kontrol)\n    for standard, variants in STANDARD_WORDS.items():\n        if word in variants:\n            word_cache[word] = standard\n            return standard\n    \n    # Fuzzy matching\n    best_match = word\n    best_score = 0\n    \n    for standard, variants in STANDARD_WORDS.items():\n        for variant in variants:\n            # Uzunluk farkı çok fazlaysa skip et (performans)\n            if abs(len(word) - len(variant)) > 3:\n                continue\n            \n            # Farklı similarity metrikleri\n            ratio_score = fuzz.ratio(word, variant)\n            partial_score = fuzz.partial_ratio(word, variant)\n            \n            max_score = max(ratio_score, partial_score)\n            \n            if max_score > best_score and max_score >= threshold:\n                best_score = max_score\n                best_match = standard\n    \n    # Cache'e kaydet\n    word_cache[word] = best_match\n    return best_match\n\ndef clean_punctuation_and_format(text):\n    \"\"\"\n    Noktalama ve format temizliği\n    \"\"\"\n    if not text:\n        return text\n    \n    # Noktalama düzeltmeleri\n    text = re.sub(r'\\.+', '.', text)  # Çoklu nokta\n    text = re.sub(r',+', ',', text)   # Çoklu virgül\n    text = re.sub(r':+', ':', text)   # Çoklu iki nokta\n    \n    # Numara formatları\n    text = re.sub(r'(\\d+)\\s*[/-]\\s*(\\d+)', r'\\1/\\2', text)  # \"15 - 3\" → \"15/3\"\n    text = re.sub(r'(\\d+)\\s*\\.\\s*(\\d+)', r'\\1/\\2', text)    # \"15 . 3\" → \"15/3\"\n    \n    # Parantez temizliği\n    text = re.sub(r'\\(\\s*\\)', '', text)  # Boş parantez\n    text = re.sub(r'\\[\\s*\\]', '', text)  # Boş köşeli parantez\n    \n    return text\ndef remove_duplicates(text):\n    \"\"\"Çift kelimeleri temizle\"\"\"\n    words = text.split()\n    result = []\n    for word in words:\n        if not result or word != result[-1]:\n            result.append(word)\n    return ' '.join(result)\n\ndef smart_address_normalization(text):\n    \"\"\"Fuzzy matching ile tam adres normalizasyonu\"\"\"\n    if pd.isna(text):\n        return text\n    \n    text = str(text).lower().strip()\n    \n    # 1. Format temizliği\n    text = clean_punctuation_and_format(text)\n    \n    # 2. Kelimelere ayır ve normalize et\n    words = text.split()\n    normalized_words = []\n    \n    for word in words:\n        # Noktalama işaretlerini ayır\n        punct_pattern = r'^([^\\w]*)(.*?)([^\\w]*)$'\n        match = re.match(punct_pattern, word)\n        \n        if match:\n            prefix_punct, clean_word, suffix_punct = match.groups()\n            \n            if clean_word:\n                # Fuzzy matching uygula\n                standardized = fuzzy_standardize_word(clean_word, threshold=75)  # 80 daha etkili olabilir\n                normalized_word = prefix_punct + standardized + suffix_punct\n                normalized_words.append(normalized_word)\n            else:\n                normalized_words.append(word)\n        else:\n            # Fallback - direkt işle\n            standardized = fuzzy_standardize_word(word, threshold=75)\n            normalized_words.append(standardized)\n    \n    # 3. Birleştir\n    result = ' '.join(normalized_words)\n    \n    # 4. Fazla boşluk temizle\n    result = re.sub(r'\\s+', ' ', result)\n    \n    # 5. Duplicate temizle\n    result = remove_duplicates(result)\n    \n    # 6. Son temizlik\n    return result.strip()\n   \n \n\ndef process_in_batches(df, input_col, output_col, batch_size=15000, description=\"\"):\n    \"\"\"\n    Güvenli batch işleme - memory yönetimi ile\n    \"\"\"\n    print(f\"📦 {description} - {len(df):,} satır {batch_size:,} batch'de işleniyor...\")\n    \n    results = []\n    total_batches = (len(df) + batch_size - 1) // batch_size\n    \n    for i in range(0, len(df), batch_size):\n        batch_start = i\n        batch_end = min(i + batch_size, len(df))\n        batch_num = (i // batch_size) + 1\n        \n        print(f\"   📊 Batch {batch_num}/{total_batches}: {batch_start:,}-{batch_end:,}\")\n        start_time = time.time()\n        \n        # Batch'i işle\n        batch_data = df[input_col].iloc[batch_start:batch_end]\n        batch_results = batch_data.apply(smart_address_normalization)\n        results.extend(batch_results.tolist())\n        \n        batch_time = time.time() - start_time\n        print(f\"     ⏱️ Batch süresi: {batch_time:.1f}s | Cache hits: {cache_hits:,} | Cache misses: {cache_misses:,}\")\n        \n        # Memory temizliği (her 5 batch'te bir)\n        if batch_num % 5 == 0:\n            gc.collect()\n            print(f\"      Memory temizlendi | Cache boyutu: {len(word_cache):,}\")\n    \n    return results\n\n# TEST - KÜÇÜK SAMPLE İLE ÖNCE\nprint(\"\\n🧪 TEST - 1000 SATIRLIK SAMPLE İLE\")\nprint(\"=\"*50)\n\n# 1000 satırlık test\ntest_sample = train.sample(1000, random_state=42)\nstart_time = time.time()\ntest_results = test_sample['address_cleaned'].apply(smart_address_normalization)\ntest_time = time.time() - start_time\n\nprint(f\"✅ 1000 satır test: {test_time:.1f} saniye\")\nprint(f\"📊 Saniye başına: {1000/test_time:.0f} satır\")\nprint(f\"🎯 Tahmini tam süre: {(len(train) + len(test)) * test_time / 1000 / 60:.1f} dakika\")\n\n# Örnek sonuçlar\nchanged_count = (test_sample['address_cleaned'] != test_results).sum()\nprint(f\"📈 Değişen adres oranı: {changed_count}/1000 (%{changed_count/10:.1f})\")\n\nprint(\"\\n📝 SAMPLE SONUÇ ÖRNEKLERİ:\")\nfor i in range(3):\n    original = test_sample.iloc[i]['address_cleaned']\n    normalized = test_results.iloc[i]\n    if original != normalized:\n        print(f\"ÖNCE:  {original}\")\n        print(f\"SONRA: {normalized}\")\n        print()\n\n# KULLANICIDAN ONAY AL\nprint(\"\\n\" + \"=\"*70)\nprint(\"🎯 FUZZY MATCHING + BATCH İLE TAM VERİYİ İŞLEMEYE HAZIR!\")\nprint(\"📊 Beklenen süre: 15-25 dakika\")\nprint(\"💾 Memory kullanımı: Güvenli (batch + cache sistem)\")\nprint(\"🎯 Kalite: En yüksek doğruluk oranı\")\nprint()\nprint(\"Devam etmek için aşağıdaki kodu çalıştırın:\")\nprint()\n\n# ANA İŞLEME KODU\nprint(\"# ANA FUZZY MATCHING İŞLEMİ\")\nprint(\"# TRAIN VERİSİ BATCH İŞLEME\")\nprint(\"print('🚀 TRAIN VERİSİ FUZZY MATCHING BAŞLIYOR...')\")\nprint(\"train_normalized_results = process_in_batches(\")\nprint(\"    df=train,\")\nprint(\"    input_col='address_cleaned',\") \nprint(\"    output_col='address_normalized',\")\nprint(\"    batch_size=15000,\")\nprint(\"    description='Train fuzzy matching'\")\nprint(\")\")\nprint(\"train['address_normalized'] = train_normalized_results\")\nprint()\n\nprint(\"# TEST VERİSİ BATCH İŞLEME\") \nprint(\"print('🚀 TEST VERİSİ FUZZY MATCHING BAŞLIYOR...')\")\nprint(\"test_normalized_results = process_in_batches(\")\nprint(\"    df=test,\")\nprint(\"    input_col='address_cleaned',\")\nprint(\"    output_col='address_normalized',\") \nprint(\"    batch_size=15000,\")\nprint(\"    description='Test fuzzy matching'\")\nprint(\")\")\nprint(\"test['address_normalized'] = test_normalized_results\")\nprint()\n\nprint(\"# SONUÇ ANALİZİ\")\nprint(\"train_changed = (train['address_cleaned'] != train['address_normalized']).sum()\")\nprint(\"test_changed = (test['address_cleaned'] != test['address_normalized']).sum()\")\nprint(\"print(f'✅ ADIM 3 TAMAMLANDI!')\")\nprint(\"print(f'Train değişen: {train_changed:,} / {len(train):,}')\")\nprint(\"print(f'Test değişen: {test_changed:,} / {len(test):,}')\")\nprint(\"print(f'Cache efficiency: %{cache_hits/(cache_hits+cache_misses)*100:.1f}')\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✅ FUZZY MATCHING + BATCH SİSTEMİ HAZIR!\")\nprint(\"🎯 Yukarıdaki kodları sırasıyla çalıştırarak devam edin.\")\nprint(\"⚡ Kaggle'da sorunsuz çalışacak, kernel çökmeyecek!\")\nprint(\"=\"*70)","metadata":{"_uuid":"d679e75c-2e1c-4463-89d2-b0b94ee72d5d","_cell_guid":"7f104776-a17a-421f-a1fe-73a328f35d9a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-19T23:58:36.336970Z","iopub.execute_input":"2025-08-19T23:58:36.337445Z","iopub.status.idle":"2025-08-19T23:58:39.592039Z","shell.execute_reply.started":"2025-08-19T23:58:36.337288Z","shell.execute_reply":"2025-08-19T23:58:39.590860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ANA FUZZY MATCHING ISLEMI\n# TRAIN VERISI BATCH ISLEME\nprint('TRAIN VERISI FUZZY MATCHING BASLIYOR...')\ntrain_normalized_results = process_in_batches(\n    df=train,\n    input_col='address_cleaned',\n    output_col='address_normalized',\n    batch_size=15000,\n    description='Train fuzzy matching'\n)\ntrain['address_normalized'] = train_normalized_results\n\n# TEST VERISI BATCH ISLEME\nprint('TEST VERISI FUZZY MATCHING BASLIYOR...')\ntest_normalized_results = process_in_batches(\n    df=test,\n    input_col='address_cleaned',\n    output_col='address_normalized',\n    batch_size=15000,\n    description='Test fuzzy matching'\n)\ntest['address_normalized'] = test_normalized_results\n\n# SONUC ANALIZI\ntrain_changed = (train['address_cleaned'] != train['address_normalized']).sum()\ntest_changed = (test['address_cleaned'] != test['address_normalized']).sum()\nprint('ADIM 3 TAMAMLANDI!')\nprint(f'Train degisen: {train_changed:,} / {len(train):,}')\nprint(f'Test degisen: {test_changed:,} / {len(test):,}')\nprint(f'Cache efficiency: %{cache_hits/(cache_hits+cache_misses)*100:.1f}')","metadata":{"_uuid":"550cc100-1a13-4fc9-94ea-bb9840184241","_cell_guid":"edf5f254-2380-442f-9aa1-3b5336eca99f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-19T23:58:39.592809Z","iopub.execute_input":"2025-08-19T23:58:39.593072Z","iopub.status.idle":"2025-08-20T00:03:09.406982Z","shell.execute_reply.started":"2025-08-19T23:58:39.593051Z","shell.execute_reply":"2025-08-20T00:03:09.406058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ADIM 4: BENZERLIK HESAPLAMA VE LABEL TAHMIN\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.neighbors import NearestNeighbors\nimport time\nimport gc\n\nprint(\"🎯 ADIM 4: BENZERLIK HESAPLAMA VE LABEL TAHMIN\")\nprint(\"=\"*70)\n\n# VERİ DURUMU KONTROL\nprint(\"📊 Veri durumu kontrol:\")\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\nprint(f\"Train'de address_normalized var mı: {'address_normalized' in train.columns}\")\nprint(f\"Test'te address_normalized var mı: {'address_normalized' in test.columns}\")\n\n# LABEL ANALIZI\nprint(f\"\\n🏷️ Label analizi:\")\nunique_labels = train['label'].nunique()\nprint(f\"Unique label sayısı: {unique_labels:,}\")\nprint(f\"En yaygın 5 label:\")\nprint(train['label'].value_counts().head())\n\n# STRATEJI 1: TF-IDF + COSINE SIMILARITY\nprint(f\"\\n📈 STRATEJI 1: TF-IDF + COSINE SIMILARITY\")\nprint(\"=\"*50)\n\ndef create_label_representatives(train_df):\n    \"\"\"\n    Her label için temsilci adres(ler) oluştur\n    \"\"\"\n    print(\"🏗️ Label temsilcileri oluşturuluyor...\")\n    \n    label_representatives = {}\n    \n    for label in train_df['label'].unique():\n        label_addresses = train_df[train_df['label'] == label]['address_normalized'].tolist()\n        \n        if len(label_addresses) == 1:\n            # Tek adres varsa direkt kullan\n            label_representatives[label] = label_addresses[0]\n        else:\n            # Çok adres varsa en uzun olanı al (daha detaylı)\n            longest_address = max(label_addresses, key=len)\n            label_representatives[label] = longest_address\n    \n    return label_representatives\n\n# Label temsilcilerini oluştur\nlabel_reps = create_label_representatives(train)\nprint(f\"✅ {len(label_reps):,} label temsilcisi oluşturuldu\")\n\n# TF-IDF vektörleştirme\nprint(\"\\n🔢 TF-IDF vektörleştirme...\")\nvectorizer = TfidfVectorizer(\n    ngram_range=(1, 2),  # 1-gram ve 2-gram\n    min_df=2,            # En az 2 dokümanda geçmeli\n    max_df=0.8,          # Maksimum %80 dokümanda geçebilir\n    strip_accents='unicode',\n    lowercase=True\n)\n\n# Tüm adresleri birleştir (train temsilcileri + test)\nall_addresses = list(label_reps.values()) + test['address_normalized'].tolist()\nprint(f\"📝 {len(all_addresses):,} adres vektörleştiriliyor...\")\n\n# TF-IDF fit et\ntfidf_matrix = vectorizer.fit_transform(all_addresses)\nprint(f\"✅ TF-IDF matrix: {tfidf_matrix.shape}\")\n\n# Train temsilcileri ve test adreslerini ayır\nn_train_reps = len(label_reps)\ntrain_tfidf = tfidf_matrix[:n_train_reps]\ntest_tfidf = tfidf_matrix[n_train_reps:]\n\nprint(f\"📊 Train temsilci matrix: {train_tfidf.shape}\")\nprint(f\"📊 Test matrix: {test_tfidf.shape}\")\n\n# STRATEJI 2: NEAREST NEIGHBORS APPROACH\nprint(f\"\\n🎯 STRATEJI 2: NEAREST NEIGHBORS\")\nprint(\"=\"*50)\n\n# K-Nearest Neighbors modeli\nprint(\"🔍 KNN modeli oluşturuluyor...\")\nknn_model = NearestNeighbors(\n    n_neighbors=5,       # En yakın 5 komşu\n    metric='cosine',     # Cosine benzerlik\n    algorithm='brute'    # Brute force (daha doğru)\n)\n\n# Train temsilcileri ile fit et\nknn_model.fit(train_tfidf)\nprint(\"✅ KNN modeli eğitildi\")\n\n# Batch tahmin fonksiyonu\ndef predict_labels_batch(test_tfidf_batch, batch_start_idx, batch_size=5000):\n    \"\"\"\n    Test batch'i için label tahminleri yap\n    \"\"\"\n    print(f\"   🔮 Batch tahminler yapılıyor: {batch_start_idx:,}-{batch_start_idx+test_tfidf_batch.shape[0]:,}\")\n    \n    # En yakın komşuları bul\n    distances, indices = knn_model.kneighbors(test_tfidf_batch)\n    \n    predictions = []\n    confidence_scores = []\n    \n    for i in range(test_tfidf_batch.shape[0]):  # len() yerine .shape[0] kullan\n        # En yakın komşunun label'ı\n        closest_idx = indices[i][0]\n        label_list = list(label_reps.keys())\n        predicted_label = label_list[closest_idx]\n        \n        # Confidence skoru (1 - cosine distance)\n        confidence = 1 - distances[i][0]\n        \n        predictions.append(predicted_label)\n        confidence_scores.append(confidence)\n    \n    return predictions, confidence_scores\n\n# BATCH TAHMİN İŞLEMİ\nprint(f\"\\n🚀 BATCH TAHMİN İŞLEMİ BAŞLIYOR\")\nprint(\"=\"*50)\n\nbatch_size = 5000  # 5k test adresi her batch'te\ntotal_predictions = []\ntotal_confidences = []\n\nn_batches = (test_tfidf.shape[0] + batch_size - 1) // batch_size\n\nfor batch_idx in range(n_batches):\n    start_idx = batch_idx * batch_size\n    end_idx = min((batch_idx + 1) * batch_size, test_tfidf.shape[0])\n    \n    print(f\"📦 Batch {batch_idx + 1}/{n_batches}\")\n    \n    # Batch'i al\n    test_batch = test_tfidf[start_idx:end_idx]\n    \n    # Tahmin yap\n    batch_predictions, batch_confidences = predict_labels_batch(\n        test_batch, start_idx, batch_size\n    )\n    \n    total_predictions.extend(batch_predictions)\n    total_confidences.extend(batch_confidences)\n    \n    # Memory temizliği\n    if (batch_idx + 1) % 3 == 0:\n        gc.collect()\n\nprint(f\"✅ Tüm tahminler tamamlandı: {len(total_predictions):,}\")\n\n# SONUÇLARI TEST DataFrame'ine EKLE\ntest['predicted_label'] = total_predictions\ntest['confidence_score'] = total_confidences\n\n# İSTATİSTİKLER\nprint(f\"\\n📊 TAHMİN İSTATİSTİKLERİ\")\nprint(\"=\"*50)\n\nconfidence_mean = np.mean(total_confidences)\nconfidence_std = np.std(total_confidences)\n\nprint(f\"Ortalama confidence: {confidence_mean:.3f}\")\nprint(f\"Confidence std: {confidence_std:.3f}\")\nprint(f\"Min confidence: {min(total_confidences):.3f}\")\nprint(f\"Max confidence: {max(total_confidences):.3f}\")\n\n# Confidence dağılımı\nhigh_conf = sum(1 for c in total_confidences if c > 0.8)\nmedium_conf = sum(1 for c in total_confidences if 0.5 < c <= 0.8)\nlow_conf = sum(1 for c in total_confidences if c <= 0.5)\n\nprint(f\"\\nConfidence dağılımı:\")\nprint(f\"Yüksek (>0.8): {high_conf:,} (%{high_conf/len(total_confidences)*100:.1f})\")\nprint(f\"Orta (0.5-0.8): {medium_conf:,} (%{medium_conf/len(total_confidences)*100:.1f})\")\nprint(f\"Düşük (≤0.5): {low_conf:,} (%{low_conf/len(total_confidences)*100:.1f})\")\n\n# ÖRNEK TAHMİNLER\nprint(f\"\\n📝 ÖRNEK TAHMİNLER\")\nprint(\"=\"*50)\n\nsample_results = test.sample(5, random_state=42)\nfor idx, row in sample_results.iterrows():\n    print(f\"\\nTest ID: {row['id']}\")\n    print(f\"Adres: {row['address_normalized'][:60]}...\")\n    print(f\"Tahmin edilen label: {row['predicted_label']}\")\n    print(f\"Confidence: {row['confidence_score']:.3f}\")\n\n# SUBMISSION HAZIRLIK\nprint(f\"\\n📄 SUBMISSION DOSYASI HAZIRLIĞI\")\nprint(\"=\"*50)\n\n# Submission formatı kontrol\nprint(\"Sample submission format:\")\nprint(sample.head())\n\n# Submission dataframe oluştur\nsubmission = test[['id', 'predicted_label']].copy()\nsubmission.columns = ['id', 'label']  # sample_submission formatına uygun\n\nprint(f\"\\nSubmission shape: {submission.shape}\")\nprint(f\"Sample submission shape: {sample.shape}\")\n\n# İlk 5 satırı karşılaştır\nprint(f\"\\nHazırlanan submission:\")\nprint(submission.head())\n\nprint(f\"\\n✅ ADIM 4 TAMAMLANDI!\")\nprint(\"=\"*70)\nprint(\"🎯 BAŞARILAR:\")\nprint(\"• TF-IDF vektörleştirme ile semantik benzerlik\")\nprint(\"• KNN ile en yakın komşu bulma\")\nprint(\"• Batch processing ile memory yönetimi\")\nprint(\"• Confidence skorları ile güvenilirlik ölçümü\")\nprint(\"• Submission formatında hazır sonuçlar\")\nprint()\nprint(\"📁 YENİ KOLONLAR:\")\nprint(\"• test['predicted_label'] - Tahmin edilen label'lar\")\nprint(\"• test['confidence_score'] - Güvenilirlik skorları\")\nprint(\"• submission - Submit edilecek format\")\nprint()\nprint(\"🎯 SIRADA: SUBMISSION ve SONUÇ ANALIZI\")\nprint(\"=\"*70)","metadata":{"_uuid":"69e72d23-d605-44c2-b5eb-22c4a4f293d0","_cell_guid":"062e038f-164a-4bc8-9158-80d7dc870574","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-20T00:03:09.408387Z","iopub.execute_input":"2025-08-20T00:03:09.408688Z","iopub.status.idle":"2025-08-20T00:06:26.179747Z","shell.execute_reply.started":"2025-08-20T00:03:09.408667Z","shell.execute_reply":"2025-08-20T00:06:26.178721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bu kod ADIM 4'te zaten var, kontrol et:\nsubmission = test[['id', 'predicted_label']].copy()\nsubmission.columns = ['id', 'label']\n\n# CSV olarak kaydet\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"✅ Submission dosyası hazır!\")","metadata":{"_uuid":"5a6b3fee-2548-4497-bbe9-dde67d281ebc","_cell_guid":"1ecd4c62-c3f3-4734-b44d-6549c8bd2582","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-20T00:06:26.180743Z","iopub.execute_input":"2025-08-20T00:06:26.181617Z","iopub.status.idle":"2025-08-20T00:06:26.401904Z","shell.execute_reply.started":"2025-08-20T00:06:26.181586Z","shell.execute_reply":"2025-08-20T00:06:26.400691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#IYILESTIRME EVRESINE GECIS YAPTIK OZSİ\n# ÜÇ İYİLEŞTİRME STRATEJİSİ - GECE BOYU ÇALIŞSIN\n# Strateji 1: String Similarity (zaten çalışıyor)\n# Strateji 2: Geographic Features  \n# Strateji 3: Ensemble Methods\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom collections import Counter\nimport time\n\nprint(\"🌙 GECE BOYU 3 STRATEJİ HAZIRLIĞI\")\nprint(\"=\"*70)\n\n# =================================================================\n# STRATEJİ 2: GEOGRAPHIC FEATURES İLE İYİLEŞTİRME\n# =================================================================\n\nprint(\"\\n🗺️ STRATEJİ 2: GEOGRAPHIC FEATURES\")\nprint(\"=\"*60)\n\ndef extract_geographic_features(address):\n    \"\"\"\n    Adresden coğrafi özellikleri çıkar\n    \"\"\"\n    if pd.isna(address):\n        return {}\n    \n    address = str(address).lower()\n    \n    features = {\n        'city': None,\n        'district': None,\n        'neighborhood': None,\n        'street_type': None,\n        'building_number': None\n    }\n    \n    # Şehir tespiti (yaygın şehirler)\n    cities = ['istanbul', 'ankara', 'izmir', 'bursa', 'antalya', 'adana', 'konya', \n              'gaziantep', 'kayseri', 'mersin', 'eskişehir', 'diyarbakır', 'samsun']\n    for city in cities:\n        if city in address:\n            features['city'] = city\n            break\n    \n    # İlçe tespiti (İstanbul ilçeleri)\n    districts = ['kadıköy', 'beşiktaş', 'şişli', 'beyoğlu', 'fatih', 'üsküdar', \n                'maltepe', 'pendik', 'kartal', 'ataşehir', 'bakırköy', 'zeytinburnu']\n    for district in districts:\n        if district in address:\n            features['district'] = district\n            break\n    \n    # Mahalle tespiti\n    if 'mahallesi' in address:\n        # \"X mahallesi\" pattern'ini bul\n        mah_match = re.search(r'(\\w+)\\s+mahallesi', address)\n        if mah_match:\n            features['neighborhood'] = mah_match.group(1)\n    \n    # Sokak tipi\n    if 'caddesi' in address:\n        features['street_type'] = 'caddesi'\n    elif 'sokak' in address:\n        features['street_type'] = 'sokak'\n    elif 'bulvari' in address:\n        features['street_type'] = 'bulvari'\n    \n    # Bina numarası\n    number_match = re.search(r'numara\\s*(\\d+)', address)\n    if number_match:\n        features['building_number'] = number_match.group(1)\n    \n    return features\n\ndef geographic_similarity(test_features, train_features):\n    \"\"\"\n    Coğrafi özelliklere göre benzerlik hesapla\n    \"\"\"\n    score = 0.0\n    max_score = 0.0\n    \n    # Şehir eşleşmesi (en önemli)\n    max_score += 0.4\n    if test_features['city'] and train_features['city']:\n        if test_features['city'] == train_features['city']:\n            score += 0.4\n    \n    # İlçe eşleşmesi\n    max_score += 0.3\n    if test_features['district'] and train_features['district']:\n        if test_features['district'] == train_features['district']:\n            score += 0.3\n    \n    # Mahalle eşleşmesi\n    max_score += 0.2\n    if test_features['neighborhood'] and train_features['neighborhood']:\n        if test_features['neighborhood'] == train_features['neighborhood']:\n            score += 0.2\n    \n    # Sokak tipi eşleşmesi\n    max_score += 0.1\n    if test_features['street_type'] and train_features['street_type']:\n        if test_features['street_type'] == train_features['street_type']:\n            score += 0.1\n    \n    return score / max_score if max_score > 0 else 0.0\n\ndef predict_with_geographic_features(test_df, train_df, batch_size=3000):\n    \"\"\"\n    Coğrafi özelliklerle tahmin\n    \"\"\"\n    print(\"🗺️ Coğrafi özellik çıkarımı başlıyor...\")\n    \n    # Train features önceden hesapla\n    print(\"   Train coğrafi özellikleri hesaplanıyor...\")\n    train_geo_features = {}\n    \n    for _, row in train_df.iterrows():\n        label = row['label']\n        address = row['address_normalized']\n        features = extract_geographic_features(address)\n        \n        if label not in train_geo_features:\n            train_geo_features[label] = []\n        train_geo_features[label].append(features)\n    \n    # Her label için dominant özellikleri bul\n    print(\"   Label temsilci özellikleri oluşturuluyor...\")\n    label_dominant_features = {}\n    \n    for label, feature_list in train_geo_features.items():\n        dominant = {\n            'city': Counter([f['city'] for f in feature_list if f['city']]).most_common(1),\n            'district': Counter([f['district'] for f in feature_list if f['district']]).most_common(1),\n            'neighborhood': Counter([f['neighborhood'] for f in feature_list if f['neighborhood']]).most_common(1),\n            'street_type': Counter([f['street_type'] for f in feature_list if f['street_type']]).most_common(1)\n        }\n        \n        # En yaygın özellikleri al\n        label_dominant_features[label] = {\n            'city': dominant['city'][0][0] if dominant['city'] else None,\n            'district': dominant['district'][0][0] if dominant['district'] else None,\n            'neighborhood': dominant['neighborhood'][0][0] if dominant['neighborhood'] else None,\n            'street_type': dominant['street_type'][0][0] if dominant['street_type'] else None,\n        }\n    \n    print(f\"✅ {len(label_dominant_features)} label için coğrafi özellikler hazır\")\n    \n    # Test prediction\n    predictions = []\n    confidences = []\n    \n    total_batches = (len(test_df) + batch_size - 1) // batch_size\n    \n    for batch_idx in range(total_batches):\n        start_idx = batch_idx * batch_size\n        end_idx = min((batch_idx + 1) * batch_size, len(test_df))\n        \n        print(f\"   📦 Geo Batch {batch_idx + 1}/{total_batches}: {start_idx:,}-{end_idx:,}\")\n        \n        batch_df = test_df.iloc[start_idx:end_idx]\n        \n        for _, row in batch_df.iterrows():\n            test_address = row['address_normalized']\n            test_features = extract_geographic_features(test_address)\n            \n            best_score = 0.0\n            best_label = None\n            \n            # Her label ile karşılaştır\n            for label, train_features in label_dominant_features.items():\n                geo_score = geographic_similarity(test_features, train_features)\n                \n                if geo_score > best_score:\n                    best_score = geo_score\n                    best_label = label\n            \n            if best_label is None:\n                # Fallback - rastgele bir label\n                best_label = list(label_dominant_features.keys())[0]\n                best_score = 0.0\n            \n            predictions.append(best_label)\n            confidences.append(best_score)\n    \n    return predictions, confidences\n\n# =================================================================\n# STRATEJİ 3: ENSEMBLE METHODS (Hibrit Yaklaşım)\n# =================================================================\n\nprint(\"\\n🤝 STRATEJİ 3: ENSEMBLE METHODS\")\nprint(\"=\"*60)\n\ndef weighted_ensemble_prediction(test_df, \n                                string_preds, string_confs,\n                                geo_preds, geo_confs,\n                                weights=[0.7, 0.3]):\n    \"\"\"\n    Farklı yöntemlerin ağırlıklı kombinasyonu\n    \"\"\"\n    print(f\"🤝 Ensemble tahmin (weights: {weights})\")\n    \n    final_predictions = []\n    final_confidences = []\n    \n    for i in range(len(test_df)):\n        string_pred = string_preds[i]\n        string_conf = string_confs[i]\n        geo_pred = geo_preds[i]\n        geo_conf = geo_confs[i]\n        \n        # Ağırlıklı confidence hesapla\n        weighted_string = string_conf * weights[0]\n        weighted_geo = geo_conf * weights[1]\n        \n        # En yüksek ağırlıklı confidence'ı seç\n        if weighted_string >= weighted_geo:\n            final_predictions.append(string_pred)\n            final_confidences.append(string_conf)\n        else:\n            final_predictions.append(geo_pred)\n            final_confidences.append(geo_conf)\n    \n    return final_predictions, final_confidences\n\ndef voting_ensemble_prediction(test_df,\n                              string_preds, string_confs,\n                              geo_preds, geo_confs,\n                              confidence_threshold=0.5):\n    \"\"\"\n    Voting tabanlı ensemble\n    \"\"\"\n    print(f\"🗳️ Voting ensemble (threshold: {confidence_threshold})\")\n    \n    final_predictions = []\n    final_confidences = []\n    \n    for i in range(len(test_df)):\n        string_pred = string_preds[i]\n        string_conf = string_confs[i]\n        geo_pred = geo_preds[i]\n        geo_conf = geo_confs[i]\n        \n        # Yüksek confidence'lı tahminleri say\n        high_conf_votes = []\n        \n        if string_conf >= confidence_threshold:\n            high_conf_votes.append((string_pred, string_conf))\n        \n        if geo_conf >= confidence_threshold:\n            high_conf_votes.append((geo_pred, geo_conf))\n        \n        if high_conf_votes:\n            # En yüksek confidence'lı oyu seç\n            best_vote = max(high_conf_votes, key=lambda x: x[1])\n            final_predictions.append(best_vote[0])\n            final_confidences.append(best_vote[1])\n        else:\n            # Hiç yüksek confidence yoksa string'i tercih et\n            final_predictions.append(string_pred)\n            final_confidences.append(string_conf)\n    \n    return final_predictions, final_confidences\n\n# =================================================================\n# OTOMATIK ÇALIŞMA PIPELINE'I\n# =================================================================\n\nprint(\"\\n🤖 OTOMATİK ÇALIŞMA PIPELINE'I HAZIR\")\nprint(\"=\"*70)\n\ndef run_all_strategies_pipeline():\n    \"\"\"\n    Tüm stratejileri sırayla çalıştır\n    \"\"\"\n    print(\"🚀 PIPELINE BAŞLIYOR - GECE BOYU ÇALIŞACAK\")\n    start_time = time.time()\n    \n    # Strateji 1: String Similarity (zaten çalışıyor - bekle)\n    print(\"\\n1️⃣ String Similarity bekleniyor...\")\n    print(\"   (Manuel olarak tamamlanmasını bekle)\")\n    \n    # Strateji 2: Geographic Features\n    print(\"\\n2️⃣ Geographic Features başlıyor...\")\n    geo_start = time.time()\n    \n    geo_predictions, geo_confidences = predict_with_geographic_features(\n        test, train, batch_size=3000\n    )\n    \n    geo_time = time.time() - geo_start\n    print(f\"✅ Geographic Features tamamlandı: {geo_time/60:.1f} dakika\")\n    \n    # Sonuçları kaydet\n    test['geo_predicted_label'] = geo_predictions\n    test['geo_confidence'] = geo_confidences\n    \n    # Geographic submission\n    geo_submission = test[['id', 'geo_predicted_label']].copy()\n    geo_submission.columns = ['id', 'label']\n    geo_submission.to_csv('/kaggle/working/submission_geographic.csv', index=False)\n    print(\"💾 Geographic submission kaydedildi\")\n    \n    # İstatistikler\n    geo_conf_mean = np.mean(geo_confidences)\n    geo_high_conf = sum(1 for c in geo_confidences if c > 0.6)\n    \n    print(f\"📊 Geographic confidence stats:\")\n    print(f\"   Ortalama: {geo_conf_mean:.3f}\")\n    print(f\"   Yüksek (>0.6): {geo_high_conf:,} (%{geo_high_conf/len(geo_confidences)*100:.1f})\")\n    \n    total_time = time.time() - start_time\n    print(f\"\\n⏰ Toplam süre: {total_time/60:.1f} dakika\")\n    print(\"\\n🎯 PIPELINE HAZIR - STRING SIMILARITY BİTİNCE ENSEMBLE YAPILACAK\")\n\ndef run_ensemble_when_ready():\n    \"\"\"\n    String similarity bitince ensemble yap\n    \"\"\"\n    print(\"\\n3️⃣ ENSEMBLE METHODS BAŞLIYOR...\")\n    \n    # String similarity sonuçlarını kontrol et\n    if 'predicted_label_v2' not in test.columns:\n        print(\"❌ String similarity henüz tamamlanmamış!\")\n        return\n    \n    # Ensemble predictions\n    print(\"🤝 Weighted Ensemble...\")\n    weighted_preds, weighted_confs = weighted_ensemble_prediction(\n        test,\n        test['predicted_label_v2'], test['confidence_v2'],\n        test['geo_predicted_label'], test['geo_confidence'],\n        weights=[0.7, 0.3]\n    )\n    \n    print(\"🗳️ Voting Ensemble...\")\n    voting_preds, voting_confs = voting_ensemble_prediction(\n        test,\n        test['predicted_label_v2'], test['confidence_v2'],\n        test['geo_predicted_label'], test['geo_confidence'],\n        confidence_threshold=0.4\n    )\n    \n    # Sonuçları kaydet\n    test['weighted_ensemble_pred'] = weighted_preds\n    test['weighted_ensemble_conf'] = weighted_confs\n    test['voting_ensemble_pred'] = voting_preds\n    test['voting_ensemble_conf'] = voting_confs\n    \n    # Ensemble submissions\n    weighted_submission = test[['id', 'weighted_ensemble_pred']].copy()\n    weighted_submission.columns = ['id', 'label']\n    weighted_submission.to_csv('/kaggle/working/submission_weighted_ensemble.csv', index=False)\n    \n    voting_submission = test[['id', 'voting_ensemble_pred']].copy()\n    voting_submission.columns = ['id', 'label']\n    voting_submission.to_csv('/kaggle/working/submission_voting_ensemble.csv', index=False)\n    \n    print(\"💾 Ensemble submissions kaydedildi\")\n    \n    # Final stats\n    print(f\"\\n📊 FINAL CONFIDENCE KARŞILAŞTIRMASI:\")\n    print(f\"String Similarity: {np.mean(test['confidence_v2']):.3f}\")\n    print(f\"Geographic: {np.mean(test['geo_confidence']):.3f}\")\n    print(f\"Weighted Ensemble: {np.mean(weighted_confs):.3f}\")\n    print(f\"Voting Ensemble: {np.mean(voting_confs):.3f}\")\n\n# OTOMATIK BAŞLATMA\nprint(\"\\n🌙 GECE BOYUNCA ÇALIŞMA BAŞLIYOR...\")\nprint(\"=\"*70)\nprint(\"1️⃣ String Similarity devam ediyor...\")\nprint(\"2️⃣ Geographic Features başlatılıyor...\")\n\n# Geographic strategy'yi başlat\nrun_all_strategies_pipeline()\n\nprint(\"\\n🎯 SABAH KALKTIKTA YAPILACAKLAR:\")\nprint(\"=\"*50)\nprint(\"1. String similarity tamamlandıysa kontrol et\")\nprint(\"2. Aşağıdaki kodu çalıştır:\")\nprint()\nprint(\"# ENSEMBLE COMPLETION\")\nprint(\"run_ensemble_when_ready()\")\nprint()\nprint(\"3. Submission dosyalarını kontrol et:\")\nprint(\"   - submission_v2.csv (String)\")\nprint(\"   - submission_geographic.csv (Geographic)\")\nprint(\"   - submission_weighted_ensemble.csv (Ensemble)\")\nprint(\"   - submission_voting_ensemble.csv (Ensemble)\")\nprint()\nprint(\"4. En iyi performans gösteren submission'ı seç!\")\nprint()\nprint(\"😴 İyi uykular! Sistem gece boyu çalışacak...\")\nprint(\"=\"*70)","metadata":{"_uuid":"78fa2926-4e59-4fb5-91d3-900b039c773f","_cell_guid":"29a2df0b-bf09-4ff4-973d-49b5c99b1b2c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-20T00:06:26.402968Z","iopub.execute_input":"2025-08-20T00:06:26.403388Z","execution_failed":"2025-08-20T11:27:56.221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test dataframe'inde hangi kolonlar var görelim\nprint(\"Test DataFrame kolonları:\")\nprint(test.columns.tolist())\nprint()\nprint(\"Test shape:\", test.shape)","metadata":{"_uuid":"9bf74dbb-447d-4799-a017-9f158723c070","_cell_guid":"55c1b3a4-a2e1-4b45-8c68-fcf556d15f9b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-08-20T11:27:56.223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Geographic vs TF-IDF ensemble\nweighted_preds, weighted_confs = weighted_ensemble_prediction(\n    test,\n    test['predicted_label'], test['confidence_score'],      # TF-IDF\n    test['geo_predicted_label'], test['geo_confidence'],    # Geographic\n    weights=[0.3, 0.7]  # Geographic'e daha çok ağırlık ver\n)\n\n# Sonucu kaydet\ntest['ensemble_pred'] = weighted_preds\ntest['ensemble_conf'] = weighted_confs\n\nensemble_submission = test[['id', 'ensemble_pred']].copy()\nensemble_submission.columns = ['id', 'label']\nensemble_submission.to_csv('/kaggle/working/submission_ensemble_v1.csv', index=False)\nprint(\"✅ Ensemble submission hazır!\")","metadata":{"_uuid":"8e6ab5d4-501a-4cd2-a845-c7c9b01d9640","_cell_guid":"97b076c2-f650-4294-94d9-a5804df60a83","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-08-20T11:27:56.224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ← BURAYA YENİ KODU YAPIŞITIR:\nprint(\"\\n🔧 Ensemble V2 - TF-IDF Ağırlıklı:\")\nweighted_preds, weighted_confs = weighted_ensemble_prediction(\n    test,\n    test['predicted_label'], test['confidence_score'],      # TF-IDF\n    test['geo_predicted_label'], test['geo_confidence'],    # Geographic  \n    weights=[0.8, 0.2]  # %80 TF-IDF, %20 Geographic\n)\n\n# Kaydet\ntest['ensemble_v2_pred'] = weighted_preds\nensemble_v2 = test[['id', 'ensemble_v2_pred']].copy()\nensemble_v2.columns = ['id', 'label']\nensemble_v2.to_csv('/kaggle/working/submission_ensemble_v2.csv', index=False)\nprint(\"✅ Ensemble V2 hazır!\")","metadata":{"_uuid":"0d7e5edb-0a7e-4967-9a6f-4e492f6f91de","_cell_guid":"3a992ff2-5b95-4680-8bc0-3a1d25da30a9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-08-20T11:27:56.224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ENTEGRASYON REHBERİ: MEVCUt KOD + ADVANCED SYSTEM\n# Mevcut kodunuzun SONUNA ekleyin\n\nprint(\"🔄 ADVANCED SYSTEM ENTEGRASYONU\")\nprint(\"=\"*70)\n\n# =====================================================================\n# PHASE 1: MEVCUT VERİLERİNİZİ KULLANARAK ADVANCED SYSTEM\n# =====================================================================\n\n# Mevcut verilerinizi kullan (train, test zaten yüklü)\n# address_normalized kolonları zaten var\n\nclass AdvancedAddressMatcher:\n    def __init__(self):\n        # Mevcut STANDARD_WORDS'ınızı genişlet\n        self.STANDARD_WORDS = {\n            'sokak': ['sok', 'sk', 's', 'sokagi', 'sokağı', 'sokag', 'sokok', 'sokk', \n                     'skk', 'soak', 'skoak', 'sokakk', 'skak', 'soks', 'sokkak'],\n            'mahallesi': ['mah', 'mh', 'm', 'maha', 'mahl', 'mahle', 'mahall', \n                         'mahalle', 'mahal', 'mhal', 'mhle', 'mhall'],\n            'caddesi': ['cad', 'cd', 'c', 'cadde', 'caddesi', 'cde', 'cdd', 'cdde'],\n            'apartmani': ['apt', 'ap', 'apartman', 'apartmani', 'apartmn'],\n            'bulvari': ['bul', 'bulvar', 'bulvari', 'bv', 'blv', 'blvr'],\n            'numara': ['no', 'n', 'num', 'numara', 'nr', 'nmr'],\n            'daire': ['d', 'da', 'dr', 'daire', 'dair', 'dare']\n        }\n        \n        # Pattern'lar\n        self.NEIGHBORHOOD_PATTERN = r'(\\w+(?:\\s+\\w+)*?)\\s*(?:mahallesi|mah|mh|mahal|mahalle)\\b'\n        self.STREET_PATTERN = r'(\\w+(?:\\s+\\w+)*?)\\s*(?:sokak|sok|sk|caddesi|cad|cd|sokağı|cadde|bulvari)\\b'\n        self.BUILDING_PATTERN = r'\\b(?:no:?\\s*)?(\\d+(?:[/-]\\d+)?)\\b'\n        \n        # Lookup tables\n        self.exact_fingerprints = {}\n        self.partial_matches = defaultdict(list)\n        self.cache = {}\n    \n    def normalize_word(self, word, threshold=85):\n        \"\"\"Mevcut normalize fonksiyonunuzla aynı ama daha strict\"\"\"\n        if not word or len(word) < 2:\n            return word\n            \n        word = word.lower().strip()\n        \n        if word.isdigit():\n            return word\n            \n        if word in self.cache:\n            return self.cache[word]\n        \n        # Exact match\n        for standard, variants in self.STANDARD_WORDS.items():\n            if word in variants or word == standard:\n                self.cache[word] = standard\n                return standard\n        \n        # Fuzzy match (threshold artırıldı)\n        best_match = word\n        best_score = 0\n        \n        for standard, variants in self.STANDARD_WORDS.items():\n            for variant in variants:\n                if abs(len(word) - len(variant)) > 2:\n                    continue\n                \n                score = fuzz.ratio(word, variant)\n                if score > best_score and score >= threshold:\n                    best_score = score\n                    best_match = standard\n        \n        self.cache[word] = best_match\n        return best_match\n    \n    def extract_components(self, address):\n        \"\"\"Adresi componentlere ayır\"\"\"\n        if not address:\n            return {'neighborhood': '', 'street': '', 'building': ''}\n        \n        address = str(address).lower()\n        \n        components = {}\n        \n        # Mahalle\n        match = re.search(self.NEIGHBORHOOD_PATTERN, address)\n        components['neighborhood'] = self.normalize_word(match.group(1)) if match else ''\n        \n        # Sokak\n        match = re.search(self.STREET_PATTERN, address)\n        components['street'] = self.normalize_word(match.group(1)) if match else ''\n        \n        # Bina no\n        match = re.search(self.BUILDING_PATTERN, address)\n        components['building'] = match.group(1) if match else ''\n        \n        return components\n    \n    def create_fingerprint(self, address):\n        \"\"\"Address fingerprint oluştur\"\"\"\n        components = self.extract_components(address)\n        fingerprint = f\"{components['neighborhood']}|{components['street']}|{components['building']}\"\n        return fingerprint.replace('||', '|').strip('|')\n    \n    def build_database(self, train_df):\n        \"\"\"Train verisiyle database oluştur\"\"\"\n        print(\"🏗️ Advanced database oluşturuluyor...\")\n        \n        for _, row in train_df.iterrows():\n            address = row['address_normalized']\n            label = row['label']\n            \n            fingerprint = self.create_fingerprint(address)\n            components = self.extract_components(address)\n            \n            # Exact match table\n            if fingerprint not in self.exact_fingerprints:\n                self.exact_fingerprints[fingerprint] = []\n            self.exact_fingerprints[fingerprint].append(label)\n            \n            # Partial match table\n            if components['neighborhood'] and components['street']:\n                partial_key = f\"{components['neighborhood']}|{components['street']}\"\n                self.partial_matches[partial_key].append((label, components['building']))\n        \n        # En yaygın label'ları seç\n        for fp in self.exact_fingerprints:\n            labels = self.exact_fingerprints[fp]\n            most_common = Counter(labels).most_common(1)[0][0]\n            self.exact_fingerprints[fp] = most_common\n        \n        print(f\"✅ {len(self.exact_fingerprints)} exact fingerprint hazır\")\n        print(f\"✅ {len(self.partial_matches)} partial match hazır\")\n    \n    def predict_single(self, address):\n        \"\"\"Tek adres için gelişmiş tahmin\"\"\"\n        fingerprint = self.create_fingerprint(address)\n        components = self.extract_components(address)\n        \n        # Level 1: Exact match\n        if fingerprint in self.exact_fingerprints:\n            return self.exact_fingerprints[fingerprint], 0.95, \"exact\"\n        \n        # Level 2: Partial match (mahalle + sokak)\n        if components['neighborhood'] and components['street']:\n            partial_key = f\"{components['neighborhood']}|{components['street']}\"\n            \n            if partial_key in self.partial_matches:\n                candidates = self.partial_matches[partial_key]\n                \n                # Bina numarası varsa exact match dene\n                if components['building']:\n                    for label, building in candidates:\n                        if building == components['building']:\n                            return label, 0.85, \"partial_exact\"\n                \n                # En yaygın label'ı al\n                labels = [label for label, _ in candidates]\n                most_common = Counter(labels).most_common(1)[0][0]\n                return most_common, 0.75, \"partial_common\"\n        \n        # Level 3: Fuzzy mahalle match\n        if components['neighborhood']:\n            for partial_key in self.partial_matches:\n                key_neighborhood = partial_key.split('|')[0]\n                similarity = fuzz.ratio(components['neighborhood'], key_neighborhood)\n                \n                if similarity >= 85:  # Yüksek threshold\n                    candidates = self.partial_matches[partial_key]\n                    labels = [label for label, _ in candidates]\n                    most_common = Counter(labels).most_common(1)[0][0]\n                    return most_common, 0.6, \"fuzzy\"\n        \n        # Level 4: Fallback - mevcut TF-IDF sonucunu kullan\n        return None, 0.0, \"fallback\"\n    \n    def predict_batch(self, test_addresses, batch_size=5000):\n        \"\"\"Batch prediction\"\"\"\n        predictions = []\n        confidences = []\n        match_types = []\n        \n        print(f\"🔮 Advanced prediction için {len(test_addresses):,} adres işleniyor...\")\n        \n        for i in range(0, len(test_addresses), batch_size):\n            batch = test_addresses[i:i+batch_size]\n            batch_num = i // batch_size + 1\n            total_batches = (len(test_addresses) + batch_size - 1) // batch_size\n            \n            print(f\"   📦 Batch {batch_num}/{total_batches}\")\n            \n            for address in batch:\n                pred, conf, match_type = self.predict_single(address)\n                predictions.append(pred)\n                confidences.append(conf)\n                match_types.append(match_type)\n        \n        return predictions, confidences, match_types\n\n# =====================================================================\n# PHASE 2: SMART ENSEMBLE (MEVCUT + ADVANCED)\n# =====================================================================\n\ndef smart_ensemble_with_existing(test_df, \n                                existing_preds, existing_confs,\n                                advanced_preds, advanced_confs, advanced_types):\n    \"\"\"Mevcut TF-IDF + Advanced sistem ensemble\"\"\"\n    \n    print(\"🤝 Smart ensemble: Mevcut TF-IDF + Advanced system\")\n    \n    final_predictions = []\n    final_confidences = []\n    \n    for i in range(len(test_df)):\n        existing_pred = existing_preds[i]\n        existing_conf = existing_confs[i]\n        \n        advanced_pred = advanced_preds[i] \n        advanced_conf = advanced_confs[i]\n        advanced_type = advanced_types[i]\n        \n        # Decision logic\n        if advanced_type == \"exact\":\n            # Exact match'e güven\n            final_predictions.append(advanced_pred)\n            final_confidences.append(advanced_conf)\n        elif advanced_type == \"partial_exact\":\n            # Partial exact'e güven\n            final_predictions.append(advanced_pred)\n            final_confidences.append(advanced_conf)\n        elif advanced_type == \"partial_common\":\n            # Confidence'a göre karar ver\n            if advanced_conf > existing_conf:\n                final_predictions.append(advanced_pred)\n                final_confidences.append(advanced_conf)\n            else:\n                final_predictions.append(existing_pred)\n                final_confidences.append(existing_conf)\n        elif advanced_type == \"fuzzy\":\n            # Mevcut TF-IDF'e güven (semantic similarity için)\n            final_predictions.append(existing_pred)\n            final_confidences.append(existing_conf)\n        else:  # fallback\n            # Mevcut TF-IDF sonucu kullan\n            final_predictions.append(existing_pred)\n            final_confidences.append(existing_conf)\n    \n    return final_predictions, final_confidences\n\n# =====================================================================\n# PHASE 3: EXECUTION - MEVCUt KODUNUZDAKİ VERİLERİ KULLAN\n# =====================================================================\n\ndef run_advanced_on_existing_data():\n    \"\"\"Mevcut verileriniz üzerinde advanced sistemi çalıştır\"\"\"\n    \n    print(\"\\n🚀 ADVANCED SYSTEM - MEVCUt VERİLER ÜZERİNDE\")\n    print(\"=\"*70)\n    \n    # Mevcut verilerinizi kontrol et\n    if 'predicted_label' not in test.columns:\n        print(\"❌ Mevcut TF-IDF tahminleri bulunamadı!\")\n        print(\"Önce mevcut kodunuzu çalıştırın, sonra bu sistemi çalıştırın\")\n        return\n    \n    # Advanced matcher oluştur\n    advanced_matcher = AdvancedAddressMatcher()\n    \n    # Database oluştur\n    advanced_matcher.build_database(train)\n    \n    # Advanced predictions\n    test_addresses = test['address_normalized'].tolist()\n    advanced_preds, advanced_confs, advanced_types = advanced_matcher.predict_batch(test_addresses)\n    \n    # Mevcut sonuçlarla ensemble\n    existing_preds = test['predicted_label'].tolist()\n    existing_confs = test['confidence_score'].tolist()\n    \n    final_preds, final_confs = smart_ensemble_with_existing(\n        test, existing_preds, existing_confs,\n        advanced_preds, advanced_confs, advanced_types\n    )\n    \n    # Sonuçları ekle\n    test['advanced_prediction'] = advanced_preds\n    test['advanced_confidence'] = advanced_confs\n    test['advanced_match_type'] = advanced_types\n    test['final_ensemble_prediction'] = final_preds\n    test['final_ensemble_confidence'] = final_confs\n    \n    # Performance analizi\n    print(f\"\\n📊 ADVANCED SYSTEM PERFORMANCE:\")\n    print(\"=\"*50)\n    \n    match_type_stats = Counter(advanced_types)\n    for match_type, count in match_type_stats.items():\n        pct = count / len(advanced_types) * 100\n        print(f\"{match_type}: {count:,} ({pct:.1f}%)\")\n    \n    # Confidence karşılaştırması\n    print(f\"\\n📈 CONFIDENCE KARŞILAŞTIRMASI:\")\n    print(f\"Mevcut TF-IDF ortalama: {np.mean(existing_confs):.3f}\")\n    print(f\"Advanced system ortalama: {np.mean(advanced_confs):.3f}\")\n    print(f\"Final ensemble ortalama: {np.mean(final_confs):.3f}\")\n    \n    # High confidence oranları\n    existing_high = sum(1 for c in existing_confs if c > 0.7)\n    advanced_high = sum(1 for c in advanced_confs if c > 0.7)\n    final_high = sum(1 for c in final_confs if c > 0.7)\n    \n    print(f\"\\nYüksek confidence (>0.7) oranları:\")\n    print(f\"Mevcut: {existing_high:,} ({existing_high/len(existing_confs)*100:.1f}%)\")\n    print(f\"Advanced: {advanced_high:,} ({advanced_high/len(advanced_confs)*100:.1f}%)\")\n    print(f\"Final: {final_high:,} ({final_high/len(final_confs)*100:.1f}%)\")\n    \n    # Submission dosyaları oluştur\n    submissions = [\n        ('submission_advanced_only.csv', 'advanced_prediction'),\n        ('submission_final_ensemble.csv', 'final_ensemble_prediction')\n    ]\n    \n    for filename, column in submissions:\n        submission = test[['id', column]].copy()\n        submission.columns = ['id', 'label']\n        submission.to_csv(f'/kaggle/working/{filename}', index=False)\n        print(f\"💾 {filename} kaydedildi\")\n    \n    print(f\"\\n✅ ADVANCED SYSTEM TAMAMLANDI!\")\n    print(\"=\"*70)\n    print(\"🎯 EXPECTED IMPROVEMENT:\")\n    print(\"• Exact + Partial matches: Büyük doğruluk artışı\")\n    print(\"• Smart ensemble: En iyi iki sistemin kombinasyonu\") \n    print(\"• Beklenen doğruluk: %40-70 (mevcut %14'ten)\")\n    print(\"\\n🚀 Test etmek için submission_final_ensemble.csv'yi kullanın!\")\n\n# =====================================================================\n# ÇALIŞTIR\n# =====================================================================\n\n# Bu fonksiyonu çalıştırın\nrun_advanced_on_existing_data()","metadata":{"_uuid":"3224dfe8-518c-4f82-8b4a-71877b3d412b","_cell_guid":"7e11399a-41a1-4cb1-85c9-14e03a2c728e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-08-20T11:27:56.224Z"}},"outputs":[],"execution_count":null}]}